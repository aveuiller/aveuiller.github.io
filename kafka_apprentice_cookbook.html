<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
            <meta name="robots" content="index, follow">
        <title>
Apache Kafka: Apprentice Cookbook        | Antoine Veuiller</title>
        <meta name="author" content="Antoine Veuiller">
        <meta name="generator" content="Pelican v4.8.0">
            <link rel="stylesheet" href="https://aveuiller.github.io/theme/css/main.min.css?4566fab6">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-icons/1.10.5/font/bootstrap-icons.min.css" integrity="sha512-ZnR2wlLbSbr8/c9AgLg3jQPAattCUImNsae6NHYnS9KrIwRdcY9DxFotXhNAKIKbAXlRnujIqUWoXXwqyFOeIQ==" crossorigin="anonymous" referrerpolicy="no-referrer">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css" integrity="sha512-+EoPw+Fiwh6eSeRK7zwIKG2MA8i3rV/DGa3tdttQGgWyatG/SkncT53KHQaS5Jh9MNOT3dmFL0FjTY08And/Cw==" crossorigin="anonymous" referrerpolicy="no-referrer">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F9565WT2VE"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'G-F9565WT2VE');
    </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.3/jquery.min.js" integrity="sha512-STof4xm1wgkfm7heWqFJVn58Hm3EtS31XFaagaa8VMReCXAkQnJZ+jEy8PCC/iT18dFy95WcExNHFTqLyp72eQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js" integrity="sha512-VK2zcvntEufaimc+efOYi622VN5ZacdnufnmX7zIhCPmjhKnOi9ZDMtg1/ug5l183f19gG1/cBstPO4D8N/Img==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js" integrity="sha512-IsNh5E3eYy3tr/JiX2Yx4vsCujtkhwl7SLqgnwLNgf04Hrt9BT9SXlLlZlWx+OK4ndzAoALhsMNcCmkggjZB1w==" crossorigin="anonymous"></script>
            <script src="https://aveuiller.github.io/theme/js/main.min.js?b6b3f836"></script>
        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">
        <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
        <meta name="msapplication-TileColor" content="#ffc40d">
        <meta name="theme-color" content="#ffffff">
        <!-- Feeds -->
            <link href="https://aveuiller.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Antoine Veuiller - All posts - Atom Feed">
            <link href="https://aveuiller.github.io/feeds/software-engineering.atom.xml" type="application/atom+xml" rel="alternate" title="Antoine Veuiller - Category: Software Engineering - Atom Feed">
        <meta name="keywords" content="Cheat sheet, Kafka">
    </head>
    <body class="bg-transparent pt-4">
        <div class="container">
                <a href="https://aveuiller.github.io" class="avatar-container float-start mx-4">
                    <div class="avatar ">
                        <div class="side">
                            <img src="/images/profile.jpg" class="img-fluid">
                        </div>
                    </div>
                </a>
            <h1>
                <a href="https://aveuiller.github.io" class="text-body-emphasis text-decoration-none">Antoine Veuiller</a>
                <small class="text-body-tertiary"><small></small></small>
            </h1>
            <nav class="navbar d-block navbar-expand-lg bg-body-tertiary shadow rounded-3">
                <a class="navbar-brand d-none" href="https://aveuiller.github.io" title="">Antoine Veuiller</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#plumage-navbar-collapse-1" aria-controls="plumage-navbar-collapse-1" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse mx-4" id="plumage-navbar-collapse-1">
                    <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item ">
                                    <a class="nav-link" href="https://aveuiller.github.io/pages/about-me.html">
                                        About Me
                                    </a>
                                </li>
<li class="nav-item ">
                                    <a class="nav-link" href="https://aveuiller.github.io/pages/collaborations.html">
                                        Collaborations
                                    </a>
                                </li>
<li class="nav-item ">
                                    <a class="nav-link" href="https://aveuiller.github.io/category/around-computer-science.html">
                                        Around Computer Science
                                    </a>
                                </li>
<li class="nav-item ">
                                    <a class="nav-link" href="https://aveuiller.github.io/category/software-engineering.html">
                                        Software Engineering
<span class="visually-hidden">(current)</span>                                    </a>
                                </li>
                    </ul>
                </div>
            </nav>
        </div>
        <div class="container mt-5">
            <div class="row">
                <div class="  col-md-9  ">
    <h1>
        <a href="https://aveuiller.github.io/kafka_apprentice_cookbook.html" rel="bookmark" title="Permalink to Apache Kafka: Apprentice Cookbook">Apache Kafka: Apprentice Cookbook</a>
    </h1>
                </div>
            </div>
            <div class="row">
                <main id="content" role="main" class="  col-md-9 ">
    
    <section id="availability-disclaimer">
<h2>Availability Disclaimer</h2>
<p>This article can be found on other sources:</p>
<ul class="simple">
<li><p>Hacker Noon: <a class="reference external" href="https://hackernoon.com/the-apprentices-guide-to-apache-kafka-n31w35ef">link</a></p></li>
<li><p>Medium: <a class="reference external" href="https://aveuiller.medium.com/557439273cee">link</a></p></li>
<li><p>Dev.to: <a class="reference external" href="https://dev.to/aveuiller/apache-kafka-apprentice-cookbook-26m">link</a></p></li>
</ul>
<hr class="docutils">
<p><img alt="Apache Kafka Logo" src="/images/posts/2021-06-01_Kafka-Apprentice-Cookbook/kafka_logo.png" class="img-fluid border rounded shadow image-process-article-photo"></p>
<p>Apache Kafka is a distributed event streaming platform built over strong concepts.
Let’s dive into the possibilities it offers.</p>
<p><a class="reference external" href="https://kafka.apache.org/">Apache Kafka</a> is a distributed event streaming platform built with an emphasis on reliability,
performance, and customization. Kafka can send and receive messages in a <a class="reference external" href="https://aws.amazon.com/pub-sub-messaging/">publish-subscribe</a> fashion.
To achieve this, the ecosystem relies on few but strong basic concepts,
which enable the community to build many features solving <a class="reference external" href="https://kafka.apache.org/uses">numerous use cases</a>, for instance:</p>
<ul class="simple">
<li><p>Processing messages as an <a class="reference external" href="https://www.confluent.io/blog/apache-kafka-vs-enterprise-service-bus-esb-friends-enemies-or-frenemies/">Enterprise Service Bus</a>.</p></li>
<li><p>Tracking Activity, metrics, and telemetries.</p></li>
<li><p>Processing Streams.</p></li>
<li><p>Supporting <a class="reference external" href="https://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/">Event sourcing</a>.</p></li>
<li><p>Storing logs.</p></li>
</ul>
<p>This article will see the concepts backing up Kafka and the different tools available to handle data streams.</p>
</section>
<section id="architecture">
<h2>Architecture</h2>
<p>The behaviour of Kafka is pretty simple: <strong>Producers</strong> push <em>Messages</em> into a particular <em>Topic</em>,
and <strong>Consumers</strong> subscribe to this <em>Topic</em> to fetch and process the <em>Messages</em>.
Let’s see how it is achieved by this technology.</p>
<section id="infrastructure-side">
<h3>Infrastructure side</h3>
<p>Independently of the use, the following components will be deployed:</p>
<ul class="simple">
<li><p>One or more <strong>Producers</strong> sending messages to the brokers.</p></li>
<li><p>One or more Kafka <strong>Brokers</strong>, the actual messaging server handling communication between producers and consumers.</p></li>
<li><p>One or more <strong>Consumers</strong> fetching and processing messages, in clusters named <strong>Consumer Groups</strong>.</p></li>
<li><p>One or more <a class="reference external" href="https://zookeeper.apache.org/"><strong>Zookeeper</strong></a> instances managing the brokers.</p></li>
<li><p>(Optionally) One or more <strong>Registry</strong> instances uniformizing message schema.</p></li>
</ul>
<p>As a scalable distributed system, Kafka is heavily relying on the concept of <em>clusters</em>.
As a result, on typical production deployment, there will likely be multiple instances of each component.</p>
<p>A <strong>Consumer Group</strong> is a cluster of the same consumer application.
This concept is heavily used by Kafka to balance the load on the applicative side of things.</p>
<p><img alt="Kafka Architecture" src="/images/posts/2021-06-01_Kafka-Apprentice-Cookbook/kafka_architecture.svg" class="img-fluid border rounded shadow image-process-article-photo"></p>
<p><em>Note: The dependency on Zookeeper will be removed soon, Cf.</em> <a class="reference external" href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum"><em>KIP-500</em></a></p>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://kafka.apache.org/documentation/#majordesignelements">Design &amp; Implementation Documentation</a></p>
<p class="p-2"><a class="reference external" href="https://hackernoon.com/kafka-basics-and-core-concepts-explained-dd1434dv">Kafka Basics and Core Concepts: Explained  — Aritra Das</a></p>
</blockquote>
</section>
<section id="applicative-side">
<h3>Applicative side</h3>
<p>A <strong>Message</strong> in Kafka is a <span class="docutils literal"><span class="pre">key-value</span></span> pair.
Those elements can be anything from an integer to a <a class="reference external" href="https://developers.google.com/protocol-buffers">Protobuf message</a>,
provided the right serializer and deserializer.</p>
<p>The message is sent to a <strong>Topic</strong>, which will store it as a <strong>Log</strong>.
The topic should be a collection of logs semantically related, but without a particular structure imposed.
A topic can either keep every message as a new log entry or only keep the last value for each key
(a.k.a. <a class="reference external" href="https://docs.confluent.io/platform/current/kafka/design.html#log-compaction">Compacted log</a>).</p>
<p>To take advantage of the multiple brokers, topics are <a class="reference external" href="https://en.wikipedia.org/wiki/Shard_%28database_architecture%29">sharded</a>
into <strong>Partitions</strong> by default.
Kafka will assign any received message to one partition depending on its key,
or using <a class="reference external" href="https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner">a partitioner algorithm</a> otherwise,
which results in a random assignment from the developer's point of view.
Each partition has a <strong>Leader</strong> responsible for all I/O operations, and <strong>Followers</strong> replicating the data.
A follower will take over the leader role in case of an issue with the current one.</p>
<p>The partition holds the received data in order, increasing an <strong>offset</strong> integer for each message.
However, there is no order guarantee between two partitions.
So for order-dependent data, one must ensure that they end up in the same partition by using the same key.</p>
<p>Each partition is assigned to a specific consumer from the consumer group.
This consumer is the only one fetching messages from this partition.
In case of shutdown of one customer, the brokers will <a class="reference external" href="https://medium.com/streamthoughts/understanding-kafka-partition-assignment-strategies-and-how-to-write-your-own-custom-assignor-ebeda1fc06f3">reassign partitions</a>
among the customers.</p>
<p>Being an asynchronous system, it can be hard and impactful on the performances to have every message delivered exactly one time to the consumer.
To mitigate this, Kafka provides <a class="reference external" href="https://kafka.apache.org/documentation/#semantics">different levels of guarantee</a>
on the number of times a message will be processed (<em>i.e.</em> at most once, at least once, exactly once).</p>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7">Log Compacted Topics in Apache Kafka — Seyed Morteza Mousavi</a></p>
<p class="p-2"><a class="reference external" href="https://www.youtube.com/watch?v=Vo6Mv5YPOJU&amp;amp;list=PLa7VYi0yPIH0KbnJQcMv5N9iW8HkZHztH&amp;amp;index=5">(Youtube) Apache Kafka 101: Replication — Confluent</a></p>
<p class="p-2"><a class="reference external" href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication">Replication Design Doc</a></p>
<p class="p-2"><a class="reference external" href="https://medium.com/@andy.bryant/processing-guarantees-in-kafka-12dd2e30be0e">Processing Guarantees in Details — Andy Briant</a></p>
</blockquote>
</section>
<section id="schema-and-registry">
<h3>Schema and Registry</h3>
<p>Messages are serialized when quitting a producer and deserialized when handled by the consumer.
To ensure compatibility, both must be using the same data definition.
Ensuring this can be hard considering the application evolution.
As a result, when dealing with a production system, it is recommended to use a schema to explicit a contract on the data structure.</p>
<p>To do this, Kafka provides a <strong>Registry</strong> server, storing and binding schema to topics.
Historically only <a class="reference external" href="https://avro.apache.org/docs/current/">Avro</a> was available, but the registry is now modular and can also handle
<a class="reference external" href="https://json-schema.org/">JSON</a> and <a class="reference external" href="https://developers.google.com/protocol-buffers">Protobuf</a> out of the box.</p>
<p>Once a producer sent a schema describing the data handled by its topic to the registry, other parties
(<em>i.e.</em> brokers and consumers) will fetch this schema on the registry to validate and deserialize the data.</p>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/schema-registry/index.html">Schema Registry Documentation</a></p>
<p class="p-2"><a class="reference external" href="https://aseigneurin.github.io/2018/08/02/kafka-tutorial-4-avro-and-schema-registry.html">Kafka tutorial #4-Avro and the Schema Registry— Alexis Seigneurin</a></p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#serializer-and-formatter">Serializer-Deserializer for Schema</a></p>
</blockquote>
</section>
</section>
<section id="integrations">
<h2>Integrations</h2>
<p>Kafka provides multiple ways of connecting to the brokers,
and each can be more useful than the others depending on the needs.
As a result, even if a library is an abstraction layer above another, it is not necessarily better for every use case.</p>
<section id="kafka-library">
<h3>Kafka library</h3>
<p>There are client libraries available in <a class="reference external" href="https://docs.confluent.io/platform/current/clients/index.html">numerous languages</a>
which help develop a producer and consumer easily.
We will use Java for the example below, but the concept remains identical for other languages.</p>
<p>The producer concept is to publish messages at any moment, so the code is pretty simple.</p>
<pre class="code literal-block"><code>public class Main {
  public static void main(String[] args) throws Exception {
    // Configure your producer
    Properties producerProperties = new Properties();
    producerProperties.put("bootstrap.servers", "localhost:29092");
    producerProperties.put("acks", "all");
    producerProperties.put("retries", 0);
    producerProperties.put("linger.ms", 1);
    producerProperties.put("key.serializer", "org.apache.kafka.common.serialization.LongSerializer");
    producerProperties.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
    producerProperties.put("schema.registry.url", "http://localhost:8081");
    
    // Initialize a producer
    Producer&lt;Long, AvroHelloMessage&gt; producer = new KafkaProducer&lt;&gt;(producerProperties);
    
    // Use it whenever you need
    producer.send(new AvroHelloMessage(1L, "this is a message", 2.4f, 1));
  }
}
</code></pre>
<p>The code is a bit more complex on the consumer part since the consumption loop needs to be created manually.
On the other hand, this gives more control over its behaviour.
The consumer state is automatically handled by the Kafka library.
As a result, restarting the worker will start at the most recent offset he encountered.</p>
<pre class="code literal-block"><code>public class Main {
    public static Properties configureConsumer() {
        Properties consumerProperties = new Properties();

        consumerProperties.put("bootstrap.servers", "localhost:29092");
        consumerProperties.put("group.id", "HelloConsumer");
        consumerProperties.put("key.deserializer", "org.apache.kafka.common.serialization.LongDeserializer");
        consumerProperties.put("value.deserializer", "io.confluent.kafka.serializers.KafkaAvroDeserializer");
        consumerProperties.put("schema.registry.url", "http://localhost:8081");
        // Configure Avro deserializer to convert the received data to a SpecificRecord (i.e. AvroHelloMessage)
        // instead of a GenericRecord (i.e. schema + array of deserialized data).
        consumerProperties.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

        return consumerProperties;
    }

    public static void main(String[] args) throws Exception {
        // Initialize a consumer
        final Consumer&lt;Long, AvroHelloMessage&gt; consumer = new KafkaConsumer&lt;&gt;(configureConsumer());

        // Chose the topics you will be polling from.
        // You can subscribe to all topics matching a Regex.
        consumer.subscribe(Pattern.compile("hello_topic_avro"));

        // Poll will return all messages from the current consumer offset
        final AtomicBoolean shouldStop = new AtomicBoolean(false);
        Thread consumerThread = new Thread(() -&gt; {
            final Duration timeout = Duration.ofSeconds(5);

            while (!shouldStop) {
                for (ConsumerRecord&lt;Long, AvroHelloMessage&gt; record : consumer.poll(timeout)) {
                    // Use your record
                    AvroHelloMessage value = record.value();
                }
                // Be kind to the broker while polling
                Thread.sleep(5);
            }

            consumer.close(timeout);
        });

        // Start consuming &amp;&amp; do other things
        consumerThread.start();
        // [...]

        // End consumption from customer
        shouldStop.set(true);
        consumerThread.join();
    }
}
</code></pre>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/clients/index.html">Available Libraries</a></p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html">Producer Configuration</a></p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html">Consumer Configuration</a></p>
</blockquote>
</section>
<section id="kafka-streams">
<h3>Kafka Streams</h3>
<p>Kafka Streams is built on top of the consumer library.
It continuously reads from a topic and processes the messages with code declared with a functional DSL.</p>
<p>During the processing, transitional data can be kept in structures called <a class="reference external" href="https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/kstream/KStream.html">KStream</a>
and <a class="reference external" href="https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/kstream/KTable.html">KTable</a>,
which are stored into topics. The former is equivalent to a standard topic, and the latter to a compacted topic.
Using these data stores will enable automatic tracking of the worker state by Kafka, helping to get back on track in case of restart.</p>
<p>The following code sample is extracted from the <a class="reference external" href="https://kafka.apache.org/28/documentation/streams/tutorial">tutorial provided by Apache</a>.
The code connects to a topic named <span class="docutils literal"><span class="pre">streams-plaintext-input</span></span> containing strings values, without necessarily providing keys.
The few lines configuring the <span class="docutils literal">StreamsBuilder</span> will:</p>
<ol class="arabic simple">
<li><p>Transform each message to lowercase.</p></li>
<li><p>Split the result using whitespaces as a delimiter.</p></li>
<li><p>Group previous tokens by value.</p></li>
<li><p>Count the number of tokens for each group and save the changes to a KTable named <span class="docutils literal"><span class="pre">counts-store</span></span>.</p></li>
<li><p>Stream the changes in this Ktable to send the values in a KStream named <span class="docutils literal"><span class="pre">streams-wordcount-output</span></span>.</p></li>
</ol>
<pre class="code literal-block"><code>public class Main {
  public static void main(String[] args) throws Exception {
    Properties props = new Properties();
    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");
    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:29092");
    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

    final StreamsBuilder builder = new StreamsBuilder();

    builder.&lt;String, String&gt;stream("streams-plaintext-input")
            .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+")))
            .groupBy((key, value) -&gt; value)
            .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as("counts-store"))
            .toStream()
            .to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));

    final Topology topology = builder.build();
    final KafkaStreams streams = new KafkaStreams(topology, props);
    final CountDownLatch latch = new CountDownLatch(1);

    // attach shutdown handler to catch control-c
    Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
      @Override
      public void run() {
        streams.close();
        latch.countDown();
      }
    });

    // The consumer loop is handled by the library
    streams.start();
    latch.await();
  }
}
</code></pre>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/streams/concepts.html">Kafka Streams Concepts</a></p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/streams/developer-guide/write-streams.html">Developer Guide</a></p>
<p class="p-2"><a class="reference external" href="https://medium.com/@andy.bryant/kafka-streams-work-allocation-4f31c24753cc">Kafka Stream Work Allocation — Andy Briant</a></p>
</blockquote>
</section>
<section id="kafka-connect">
<h3>Kafka Connect</h3>
<p>Kafka Connect provides a way of transforming and synchronizing data between almost any technology with the use of <strong>Connectors</strong>.
Confluent is hosting a <a class="reference external" href="https://www.confluent.io/hub/">Hub</a>, on which users can share connectors for various technologies.
This means that integrating a Kafka Connect pipeline is most of the time only a matter of configuration, without code required.
A single connector can even handle both connection sides:</p>
<ul class="simple">
<li><p>Populate a topic with data from any system: <em>i.e.</em> a <strong>Source</strong>.</p></li>
<li><p>Send data from a topic to any system: <em>i.e.</em> a <strong>Sink</strong>.</p></li>
</ul>
<p>The source will read data from CSV files in the following schema then publish them into a topic.
Concurrently, the sink will poll from the topic and insert the messages into a MongoDB database.
Each connector can run in the same or a distinct worker, and workers can be grouped into a cluster for scalability.</p>
<p><img alt="Kafka Connect Example" src="/images/posts/2021-06-01_Kafka-Apprentice-Cookbook/kafka_connect.png" class="img-fluid border rounded shadow image-process-article-photo"></p>
<p>The connector instance is created through a configuration specific to the library.
The file below is a configuration of the <a class="reference external" href="https://www.confluent.io/hub/mongodb/kafka-connect-mongodb">MongoDB connector</a>.
It asks to fetch all messages from the topic <span class="docutils literal"><span class="pre">mongo-source</span></span> to insert them into the collection <span class="docutils literal">sink</span> of the database named <span class="docutils literal"><span class="pre">kafka-connect</span></span>.
The credentials are provided from an external file, which is a feature of Kafka Connect to <a class="reference external" href="https://docs.confluent.io/platform/current/connect/security.html#externalizing-secrets">protect secrets</a>.</p>
<pre class="code json literal-block"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mongo-sink"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nt">"topics"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mongo-source"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"tasks.max"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"connector.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"com.mongodb.kafka.connect.MongoSinkConnector"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"connection.uri"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mongodb://${file:/auth.properties:username}:${file:/auth.properties:password}@mongo:27017"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"database"</span><span class="p">:</span><span class="w"> </span><span class="s2">"kafka_connect"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"collection"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sink"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"max.num.retries"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"retries.defer.timeout"</span><span class="p">:</span><span class="w"> </span><span class="s2">"5000"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"document.id.strategy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"post.processor.chain"</span><span class="p">:</span><span class="w"> </span><span class="s2">"com.mongodb.kafka.connect.sink.processor.DocumentIdAdder"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"delete.on.null.values"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"writemodel.strategy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre>
<p>Once the configuration complete, registering the connector is as easy as an HTTP call on the running <a class="reference external" href="https://docs.confluent.io/home/connect/userguide.html#configuring-and-running-workers">Kafka Connect instance</a>.
Afterwards, the service will automatically watch the data without further work required.</p>
<pre class="code shell literal-block"><code>$<span class="w"> </span>curl<span class="w"> </span>-i<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>-H<span class="w"> </span><span class="s2">"Accept:application/json"</span><span class="w"> </span>-H<span class="w">  </span><span class="s2">"Content-Type:application/json"</span><span class="w"> </span><span class="se">\
</span><span class="w">  </span>http://localhost:8083/connectors<span class="w"> </span>-d<span class="w"> </span>@sink-conf.json</code></pre>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/connect/userguide.html#connect-userguide">Getting Started Documentation</a></p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/connect/references/restapi.html">Connector Instance API Reference</a></p>
<p class="p-2"><a class="reference external" href="https://www.youtube.com/playlist?list=PLa7VYi0yPIH1MB2n2w8pMZguffCDu2L4Y">(Youtube) Tutorials Playlist — Confluent</a></p>
</blockquote>
</section>
<section id="ksql-database">
<h3>KSQL Database</h3>
<p>Ksql is somehow equivalent to Kafka Streams, except that every transformation is declared in an SQL-like language.
The server is connected to the brokers and can create <strong>Streams</strong> or <strong>Tables</strong> from topics.
Those two concepts behave in the same way as a KStream or KTable from Kafka Streams (<em>i.e.</em> respectively a topic and a compacted topic).</p>
<p>There are three types of query in the language definition:</p>
<ol class="arabic simple">
<li><p><strong>Persistent Query</strong> (<em>e.g.</em> <span class="docutils literal">CREATE TABLE &lt;name&gt; WITH <span class="pre">(...)</span></span>): Creates a new stream or table that will be automatically updated.</p></li>
<li><p><strong>Pull Query</strong> (<em>e.g.</em> <span class="docutils literal">SELECT * FROM &lt;table|stream&gt; WHERE ID = 1</span>): Behaves similarly to a standard DBMS. Fetches data as an instant snapshot and closes the connection.</p></li>
<li><p><strong>Push Query</strong> (<em>e.g.</em> <span class="docutils literal">SELECT * FROM &lt;table|stream&gt; EMIT CHANGES</span>): Requests a persistent connection to the server, asynchronously pushing updated values.</p></li>
</ol>
<p>The database can be used to browse the brokers' content. Topics can be discovered through the command <span class="docutils literal">list topics</span>, and their content displayed using <span class="docutils literal">print &lt;name&gt;</span>.</p>
<pre class="code sql literal-block"><code><span class="n">ksql</span><span class="o">&gt;</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">topics</span><span class="p">;</span><span class="w">
 </span><span class="n">Kafka</span><span class="w"> </span><span class="n">Topic</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="n">Partitions</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Partition</span><span class="w"> </span><span class="n">Replicas</span><span class="w">
</span><span class="c1">----------------------------------------------------
</span><span class="w"> </span><span class="n">hello_topic_json</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w">          </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span><span class="c1">----------------------------------------------------
</span><span class="w">
</span><span class="n">ksql</span><span class="o">&gt;</span><span class="w"> </span><span class="n">print</span><span class="w"> </span><span class="s1">'hello_topic_json'</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">beginning</span><span class="p">;</span><span class="w">
</span><span class="k">Key</span><span class="w"> </span><span class="n">format</span><span class="p">:</span><span class="w"> </span><span class="n">KAFKA_BIGINT</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">KAFKA_DOUBLE</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">KAFKA_STRING</span><span class="w">
</span><span class="n">Value</span><span class="w"> </span><span class="n">format</span><span class="p">:</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">KAFKA_STRING</span><span class="w">
</span><span class="n">rowtime</span><span class="p">:</span><span class="w"> </span><span class="mi">2021</span><span class="o">/</span><span class="mi">05</span><span class="o">/</span><span class="mi">25</span><span class="w"> </span><span class="mi">08</span><span class="p">:</span><span class="mi">44</span><span class="p">:</span><span class="mi">20</span><span class="p">.</span><span class="mi">922</span><span class="w"> </span><span class="n">Z</span><span class="p">,</span><span class="w"> </span><span class="k">key</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">:</span><span class="w"> </span><span class="err">{</span><span class="ss">"user_id"</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="ss">"message"</span><span class="p">:</span><span class="ss">"this is a message"</span><span class="p">,</span><span class="ss">"value"</span><span class="p">:</span><span class="mi">2</span><span class="p">.</span><span class="mi">4</span><span class="p">,</span><span class="ss">"version"</span><span class="p">:</span><span class="mi">1</span><span class="err">}</span><span class="w">
</span><span class="n">rowtime</span><span class="p">:</span><span class="w"> </span><span class="mi">2021</span><span class="o">/</span><span class="mi">05</span><span class="o">/</span><span class="mi">25</span><span class="w"> </span><span class="mi">08</span><span class="p">:</span><span class="mi">44</span><span class="p">:</span><span class="mi">20</span><span class="p">.</span><span class="mi">967</span><span class="w"> </span><span class="n">Z</span><span class="p">,</span><span class="w"> </span><span class="k">key</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">:</span><span class="w"> </span><span class="err">{</span><span class="ss">"user_id"</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="ss">"message"</span><span class="p">:</span><span class="ss">"this is another message"</span><span class="p">,</span><span class="ss">"value"</span><span class="p">:</span><span class="mi">2</span><span class="p">.</span><span class="mi">4</span><span class="p">,</span><span class="ss">"version"</span><span class="p">:</span><span class="mi">2</span><span class="err">}</span><span class="w">
</span><span class="n">rowtime</span><span class="p">:</span><span class="w"> </span><span class="mi">2021</span><span class="o">/</span><span class="mi">05</span><span class="o">/</span><span class="mi">25</span><span class="w"> </span><span class="mi">08</span><span class="p">:</span><span class="mi">44</span><span class="p">:</span><span class="mi">20</span><span class="p">.</span><span class="mi">970</span><span class="w"> </span><span class="n">Z</span><span class="p">,</span><span class="w"> </span><span class="k">key</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">:</span><span class="w"> </span><span class="err">{</span><span class="ss">"user_id"</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="ss">"message"</span><span class="p">:</span><span class="ss">"this is another message"</span><span class="p">,</span><span class="ss">"value"</span><span class="p">:</span><span class="mi">2</span><span class="p">.</span><span class="mi">6</span><span class="p">,</span><span class="ss">"version"</span><span class="p">:</span><span class="mi">1</span><span class="err">}</span></code></pre>
<p>The syntax to create and query a stream, or a table is very close to SQL.</p>
<pre class="code sql literal-block"><code><span class="c1">-- Let's create a table from the previous topic
</span><span class="n">ksql</span><span class="o">&gt;</span><span class="w"> </span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">messages</span><span class="w"> </span><span class="p">(</span><span class="n">user_id</span><span class="w"> </span><span class="nb">BIGINT</span><span class="w"> </span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="p">,</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">)</span><span class="w"> 
</span><span class="o">&gt;</span><span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="p">(</span><span class="n">KAFKA_TOPIC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'hello_topic_json'</span><span class="p">,</span><span class="w"> </span><span class="n">VALUE_FORMAT</span><span class="o">=</span><span class="s1">'JSON'</span><span class="p">);</span><span class="w">

</span><span class="c1">-- We can see the list and details of each table
</span><span class="n">ksql</span><span class="o">&gt;</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">tables</span><span class="p">;</span><span class="w">
 </span><span class="k">Table</span><span class="w"> </span><span class="n">Name</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Kafka</span><span class="w"> </span><span class="n">Topic</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="k">Key</span><span class="w"> </span><span class="n">Format</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="n">Format</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Windowed</span><span class="w">
</span><span class="c1">----------------------------------------------------------------------
</span><span class="w"> </span><span class="n">MESSAGES</span><span class="w">   </span><span class="o">|</span><span class="w"> </span><span class="n">hello_topic_json</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">KAFKA</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="n">JSON</span><span class="w">         </span><span class="o">|</span><span class="w"> </span><span class="k">false</span><span class="w">
</span><span class="c1">----------------------------------------------------------------------
</span><span class="w">
</span><span class="n">ksql</span><span class="o">&gt;</span><span class="w"> </span><span class="k">describe</span><span class="w"> </span><span class="n">messages</span><span class="p">;</span><span class="w">
</span><span class="n">Name</span><span class="w">                 </span><span class="p">:</span><span class="w"> </span><span class="n">MESSAGES</span><span class="w">
 </span><span class="n">Field</span><span class="w">   </span><span class="o">|</span><span class="w"> </span><span class="k">Type</span><span class="w">
</span><span class="c1">------------------------------------------
</span><span class="w"> </span><span class="n">USER_ID</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nb">BIGINT</span><span class="w">           </span><span class="p">(</span><span class="k">primary</span><span class="w"> </span><span class="k">key</span><span class="p">)</span><span class="w">
 </span><span class="n">MESSAGE</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="n">STRING</span><span class="p">)</span><span class="w">
</span><span class="c1">------------------------------------------
</span><span class="k">For</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="k">statistics</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="n">details</span><span class="w"> </span><span class="n">run</span><span class="p">:</span><span class="w"> </span><span class="k">DESCRIBE</span><span class="w"> </span><span class="n">EXTENDED</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Stream</span><span class="p">,</span><span class="k">Table</span><span class="o">&gt;</span><span class="p">;</span><span class="w">

</span><span class="c1">-- Appart from some additions to the language, the queries are almost declared in standard SQL. 
</span><span class="n">ksql</span><span class="o">&gt;</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">messages</span><span class="w"> </span><span class="n">EMIT</span><span class="w"> </span><span class="n">CHANGES</span><span class="p">;</span><span class="w">
</span><span class="o">+</span><span class="c1">--------+------------------------+
</span><span class="o">|</span><span class="n">USER_ID</span><span class="w"> </span><span class="o">|</span><span class="n">MESSAGE</span><span class="w">                 </span><span class="o">|</span><span class="w">
</span><span class="o">+</span><span class="c1">--------+------------------------+
</span><span class="o">|</span><span class="mi">1</span><span class="w">       </span><span class="o">|</span><span class="n">this</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">another</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="o">|</span><span class="w">
</span><span class="o">|</span><span class="mi">2</span><span class="w">       </span><span class="o">|</span><span class="n">this</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">another</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="o">|</span></code></pre>
<p>Kafka recommends using a <a class="reference external" href="https://www.confluent.io/blog/deep-dive-ksql-deployment-options/">headless ksqlDB server</a>
for production, with a file declaring all streams and tables to create.
This avoids any modification to the definitions.</p>
<p><em>Note: ksqlDB servers can be grouped in a cluster like any other consumer.</em></p>
<blockquote class="blockquote border-start border-primary-subtle bg-dark-subtle fs-6 border-4 ps-2">
<p class="p-2">Further Reading:</p>
<p class="p-2"><a class="reference external" href="https://docs.confluent.io/platform/current/streams-ksql.html">Official Documentation</a></p>
<p class="p-2"><a class="reference external" href="https://docs.ksqldb.io/en/latest/concepts/queries/">KSQL Query Types In Details</a></p>
<p class="p-2"><a class="reference external" href="https://www.youtube.com/playlist?list=PLa7VYi0yPIH2eX8q3mPpZAn3qCS1eDX8W">(Youtube) Tutorials Playlist — Confluent</a></p>
</blockquote>
</section>
</section>
<section id="conclusion">
<h2>Conclusion</h2>
<p>This article gives a broad view of the Kafka ecosystem and possibilities, which are numerous.
This article only scratches the surface of each subject.
But worry not, as they are all well documented by Apache, Confluent, and fellow developers.
Here are a few supplementary resources to dig further into Kafka:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLa7VYi0yPIH0KbnJQcMv5N9iW8HkZHztH">(Youtube) Kafka Tutorials - <em>Confluent</em></a></p></li>
<li><p><a class="reference external" href="https://kafka-tutorials.confluent.io/">Kafka Tutorials in Practice</a></p></li>
<li><p><a class="reference external" href="https://www.confluent.io/blog/5-things-every-kafka-developer-should-know/">Top 5 Things Every Apache Kafka Developer Should Know — Bill Bejeck</a></p></li>
<li><p><a class="reference external" href="https://docs.confluent.io/platform/current/app-development/kafkacat-usage.html">Kafkacat user Guide</a></p></li>
<li><p><a class="reference external" href="https://www.confluent.io/blog/troubleshooting-ksql-part-2">Troubleshooting KSQL Part 2: What’s Happening Under the Covers? — Robin Moffatt</a></p></li>
<li><p><a class="reference external" href="https://ssudan16.medium.com/kafka-internals-47e594e3f006">Apache Kafka Internals — sudan</a></p></li>
</ul>
<p><em>The complete experimental code is available on my <a class="reference external" href="https://github.com/aveuiller/frameworks-bootstrap/tree/master/Kafka">GitHub repository</a>.</em></p>
<p><em>Thanks to Sarra Habchi, and Dimitri Delabroye for the reviews</em></p>
</section>
                </main>
                    <div class="col-md-3">
    <ul class="list-group list-group-flush">
        <li class="list-group-item">
            <abbr title="2021-06-29T00:00:00+02:00">
                <i class="bi bi-calendar-event-fill"></i>
                mar. 29 juin 2021
            </abbr>
        </li>
            <li class="list-group-item">
                <address>
                    <i class="bi bi-person-circle"></i> By
                        <a href="https://aveuiller.github.io/author/antoine-veuiller.html" rel="author">Antoine Veuiller</a>
                </address>
            </li>
            <li class="list-group-item">
                <ul class="list-inline">
                                <li class="list-inline-item">    <a href="/category/software-engineering.html" rel="tag" class="badge category" data-bs-toggle="tooltip" title="6 articles with this category">Software Engineering</a></li>
                            <li class="list-inline-item">    <a href="/tag/cheat-sheet.html" rel="tag" class="badge tag" data-bs-toggle="tooltip" title="7 articles with this tag">Cheat sheet</a></li>
                            <li class="list-inline-item">    <a href="/tag/kafka.html" rel="tag" class="badge tag" data-bs-toggle="tooltip" title="7 articles with this tag">Kafka</a></li>
                </ul>
            </li>
            <li class="list-group-item">
                <nav class="nav nav-underline nav-justified">
                    <a class="nav-link " href="https://aveuiller.github.io/kubernetes_apprentice_cookbook.html" title="Kubernetes: Apprentice Cookbook" rel="prev">
                        <span aria-hidden="true">←</span> Older
                    </a>
                    <a class="nav-link " href="https://aveuiller.github.io/is_copilot_a_threat.html" title="Is GitHub Copilot a Threat to Developers? (Spoiler: It's Not)" rel="next">
                        Newer <span aria-hidden="true">→</span>
                    </a>
                </nav>
            </li>
    </ul>
                        
                    </div>
            </div>
        </div>
        <footer class="container-fluid mt-5 p-4 small fw-light">
            <div class="row mx-5">
                    <div class="col-md-2">
                            <h6>Social</h6>
                            <ul class="list-unstyled">
<li>    <a href="http://twitter.com/AVeuiller">
            <i class="bi bi-twitter"></i>
        Twitter
    </a></li><li>    <a href="https://stackoverflow.com/users/2564085/aveuiller">
            <i class="bi bi-stack-overflow"></i>
        StackOverflow
    </a></li><li>    <a href="http://github.com/aveuiller">
            <i class="bi bi-github"></i>
        GitHub
    </a></li><li>    <a href="https://aveuiller.medium.com/">
            <img src="https://www.google.com/s2/favicons?domain=aveuiller.medium.com" width="16" height="16" class="link-icon" alt="aveuiller.medium.com icon">
        Medium
    </a></li><li>    <a href="https://dev.to/aveuiller/">
            <img src="https://www.google.com/s2/favicons?domain=dev.to" width="16" height="16" class="link-icon" alt="dev.to icon">
        Dev.to
    </a></li>                            </ul>
                    </div>
                    <div class="col-md-2">
                            <h6>Professional</h6>
                            <ul class="list-unstyled">
<li>    <a href="https://linkedin.com/in/antoine-veuiller">
            <i class="bi bi-linkedin"></i>
        LinkedIn
    </a></li>                            </ul>
                    </div>
                <div class="col-md-2">
                    <h6>Browse content by</h6>
                    <ul class="list-unstyled">
                    </ul>
                </div>
                <div class="col-md-2 text-body-tertiary">
                    <h6>Copyright notice</h6>
                    <p class="small">
                        © Copyright
                        2020-2022
                        Antoine Veuiller.
                    </p>
                </div>
                <div class="col-md-2 text-body-tertiary">
                    <h6>Disclaimer</h6>
                    <p class="small">
                            All opinions expressed in this site are my own
                            personal opinions and are not endorsed by, nor
                            do they represent the opinions of my previous,
                            current and future employers or any of its
                            affiliates, partners or customers.
                    </p>
                </div>
                <div class="col-md-2">
                        <h6>Feeds</h6>
                        <ul class="list-unstyled small">
                                <li>
                                    <a href="https://aveuiller.github.io/feeds/all.atom.xml">
                                        <i class="bi  bi-rss "></i>
                                        All posts (Atom)
                                    </a>
                                </li>
                                <li>
                                    <a href="https://aveuiller.github.io/feeds/software-engineering.atom.xml">
                                        <i class="bi  bi-rss "></i>
                                        Category: Software Engineering (Atom)
                                    </a>
                                </li>
                        </ul>
                </div>
            </div>
            <div class="row mt-3">
                <div class="offset-3 col-6 small text-muted text-center">
                    Site generated by <a class="text-dark" href="https://getpelican.com">Pelican</a>.
                    <br>
                    <a class="text-dark" href="https://github.com/kdeldycke/plumage">Plumage</a>
                    theme by <a class="text-dark" href="https://kevin.deldycke.com">Kevin Deldycke</a>.
                </div>
                <div class="col-3 text-end d-flex flex-column">
                    <a class="mt-auto" href="#"><i class="bi bi-arrow-bar-up"></i> Back to top</a>
                </div>
            </div>
        </footer>
    </body>
</html>