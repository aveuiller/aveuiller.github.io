<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Antoine Veuiller</title><link href="https://aveuiller.github.io/" rel="alternate"></link><link href="https://aveuiller.github.io/feeds/all.atom.xml" rel="self"></link><id>https://aveuiller.github.io/</id><updated>2022-09-18T00:00:00+02:00</updated><subtitle></subtitle><entry><title>A Generic Approach to Troubleshooting</title><link href="https://aveuiller.github.io/troubleshooting_generic_approach.html" rel="alternate"></link><published>2022-09-18T00:00:00+02:00</published><updated>2022-09-18T00:00:00+02:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2022-09-18:/troubleshooting_generic_approach.html</id><summary type="html"></summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hacker Noon: &lt;a href="#TODO"&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Medium: &lt;a href="#TODO"&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dev.to: &lt;a href="#TODO"&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;When working with a production system, you may encounter errors at any point, originating either from an application or its underlying host.
While it may become natural for a seasoned engineer to pinpoint its origin, it can be overwhelming at times if you are unsure of where to look.&lt;/p&gt;
&lt;p&gt;This article aims to provide a generic framework to tackle production issues.
The framework will serve as a basis for any engineer aiming to learn about troubleshooting basics.
However, as with any generic framework, it may need adjustments for specific use cases.&lt;/p&gt;
&lt;h2&gt;Environment&lt;/h2&gt;
&lt;p&gt;In this article, we consider that you are working on a project that already implements the &lt;a href="https://www.pagerduty.com/resources/learn/best-practices-for-monitoring/"&gt;best practices in terms of monitoring&lt;/a&gt;
so that you have access to sensible metrics, defined for instance through the &lt;a href="https://brendangregg.com/usemethod.html"&gt;USE Method&lt;/a&gt;.
Those metrics can be used to provide performance dashboards and declare alerts triggered in case of faulty or suspicious behavior.
For a more visual reference, the following diagram shows a basic monitoring architecture using &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;, &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt;, and &lt;a href="https://prometheus.io/docs/alerting/latest/alertmanager/"&gt;Alert Manager&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Basic Monitoring Environment" src="/images/posts/2022-09-18_troubleshooting_basics/Monitoring_Environment.svg" /&gt;&lt;/p&gt;
&lt;p&gt;Upon the reception of an alert for an issue impacting a production service, the actions to take can be broken down into three steps: &lt;em&gt;Analysis&lt;/em&gt;, &lt;em&gt;Correction&lt;/em&gt;, and &lt;em&gt;Post-Mortem&lt;/em&gt;.
As some of the traces found or produced during the incident can be transient, it is recommended to keep track of any event and action that was taken on the production during the analysis and correction phases.
This will ease the post-mortem phase.&lt;/p&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;h3&gt;Explore The Issue&lt;/h3&gt;
&lt;p&gt;The first step when receiving an alert will be to find any known information about it.
Usually, a &lt;a href="https://www.pagerduty.com/resources/learn/what-is-a-runbook/"&gt;&lt;em&gt;Runbook&lt;/em&gt;&lt;/a&gt; will explain the reason why this alert fired and the steps to take to solve it.&lt;/p&gt;
&lt;p&gt;Even if the solution seems simple, you may want to find the scope of the problem.
Your response might differ if the issue is impacting one or hundreds of hosts.
Likewise, an issue crippling production will require a low response time to issue corrective action,
while a less critical issue, like a decrease in performance, may give you more time for analysis.&lt;/p&gt;
&lt;p&gt;While your production servers should behave in the same way, 
in some cases you will need to investigate the error before being able to determine the actual number of impacted hosts.&lt;/p&gt;
&lt;h3&gt;Check The Basic Metrics&lt;/h3&gt;
&lt;p&gt;If you have multiple hosts impacted, take one of them as an analysis host before you have a clear view of which metrics are important in the current case.
If you have the chance to have metrics exported to a dashboard, for instance through &lt;a href="https://github.com/prometheus/node_exporter"&gt;node-exporter&lt;/a&gt;,
you will be able to see the issue quite easily from the different indicators.
Otherwise, you will need to jump on the host and use the &lt;a href="https://linuxconfig.org/linux-basic-health-check-commands"&gt;usual tools&lt;/a&gt; to get more insights into the host's health.
In any case, you will be looking for issues with the CPU, RAM, Load, Disk, and Network.&lt;/p&gt;
&lt;p&gt;This first step will indicate if the issue is purely applicative, or if your server is over-used to some extent.
This will also help you reduce the scope of where to look further down the investigation.&lt;/p&gt;
&lt;p&gt;Regardless of the result of the previous check, you will need to look at the application-specific metrics,
both for performance through CPU and RAM usage, as well as applicative metrics such as response time, workload queues, and more.
You also need to check the applicative logs looking for anything that stands out of the ordinary.&lt;/p&gt;
&lt;p&gt;From there, if you have enough information to at least recognize the issue and perform a temporary corrective action,
you should go for it and fix the production as soon as possible before digging Further.&lt;/p&gt;
&lt;h3&gt;Dive Deeper into The Application Metrics&lt;/h3&gt;
&lt;p&gt;If you didn't find the root cause of the error yet, you need to dig into the application behavior and its interaction with the system.
This step is mostly dependent on the application, but there are some common checks that could show irregularities.&lt;/p&gt;
&lt;p&gt;On the system side, you can take a look at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The network communications and connectivity (e.g.
  &lt;a href="https://man7.org/linux/man-pages/man1/tcpdump.1.html"&gt;tcpdump&lt;/a&gt;,
  &lt;a href="https://man7.org/linux/man-pages/man8/netstat.8.html"&gt;netstat&lt;/a&gt;,
  &lt;a href="https://www.commandlinux.com/man-page/man1/telnet.1.html"&gt;telnet&lt;/a&gt;,
  &lt;a href="https://www.commandlinux.com/man-page/man8/mtr.8.html"&gt;mtr&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The Kernel and related logs (e.g. 
  &lt;a href="https://man7.org/linux/man-pages/man1/journalctl.1.html"&gt;journalctl&lt;/a&gt;,
  &lt;a href="https://man7.org/linux/man-pages/man1/dmesg.1.html"&gt;dmesg&lt;/a&gt;, …)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the applicative side, you can take a look at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The configuration through files and environment variables (e.g. &lt;a href="https://man7.org/linux/man-pages/man5/proc.5.html"&gt;/proc&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The opened file handles and connections (e.g. &lt;a href="https://man7.org/linux/man-pages/man8/lsof.8.html"&gt;lsof&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The system calls that are performed by the application (e.g. &lt;a href="https://man7.org/linux/man-pages/man1/strace.1.html"&gt;strace&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The application performances in a specific code path (e.g. &lt;a href="https://man7.org/linux/man-pages/man1/gdb.1.html"&gt;gdb&lt;/a&gt;, &lt;a href="https://github.com/google/pprof"&gt;pprof&lt;/a&gt;, …).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Correction&lt;/h2&gt;
&lt;h3&gt;Protect The Production&lt;/h3&gt;
&lt;p&gt;It is important to protect production from the issue as soon as possible.
This can take many forms but usually, you will either implement a quick fix,
that will enable the instance to still run until a long-term solution is implemented,
or isolate the faulty application from the production pools.
The latter can be done for instance by redirecting the load-balancer flow to other instances.&lt;/p&gt;
&lt;p&gt;With orchestration solutions and stateless services,
it could be appealing to simply restart the application and use a brand-new instance.
This may work in some cases, but be sure to backup all data required for further investigation before doing so,
otherwise, you may end up in the same situation later.&lt;/p&gt;
&lt;h3&gt;Implement a Long-Term Solution&lt;/h3&gt;
&lt;p&gt;Once the production is secured, you can catch your breath and start digging further into the data you collected to find the actual root cause of the alert that was triggered.
Even if the situation is stable for now, you may want to improve the overall code quality so the &lt;em&gt;future you&lt;/em&gt; don't have to investigate this issue again.&lt;/p&gt;
&lt;p&gt;This section is entirely specific to the encountered issue, the actual fix could range from host configuration or code update to a complete architectural refactor in some cases.
I would advise performing the long-term fix as soon as possible, but you may be compelled to write down the Post-Mortem beforehand if the fix is big or impacts multiple components.&lt;/p&gt;
&lt;h2&gt;Post-Mortem&lt;/h2&gt;
&lt;h3&gt;Write The Actual Post-Mortem&lt;/h3&gt;
&lt;p&gt;The main point of a &lt;a href="https://sre.google/sre-book/postmortem-culture/"&gt;Post-Mortem&lt;/a&gt; is to hold all relevant events and actions that took place during the incident.
This helps to understand the issue and improve the response process for future incidents.
It also helps fellow engineers to map potential side effects they experienced to the main incident.&lt;/p&gt;
&lt;p&gt;On top of the precise timeline, you need to describe the actual root cause from which the incident originates and the action you and your team are proposing to avoid this issue from recurring.
In case of a corrective action impacting multiple components, you will need to have this post-mortem reviewed by the right stakeholders before taking action.
The post-mortem should hold enough information for a peer to have a strong opinion about the proposal.&lt;/p&gt;
&lt;h3&gt;Improve Runbooks And Alerts&lt;/h3&gt;
&lt;p&gt;Depending on the corrective action you took, you may need to create new alerts to cover different edge cases and modify the runbook sections with up-to-date data.&lt;/p&gt;
&lt;p&gt;Even if the changes you performed are trivial, use this step as a feedback loop on the data you were missing during the investigation to help the &lt;em&gt;future you&lt;/em&gt; during the next investigation.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This article aims to provide a baseline of investigation in case of a generic production incident.
The procedure will not show you the error if you don't ask the right questions but defines a framework to help you find the right questions to ask, and where to write the answers.&lt;/p&gt;
&lt;p&gt;As a picture is worth a thousand words, here is a summary of the article through a simple flow chart.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Troubleshooting Operations Flowchart" src="/images/posts/2022-09-18_troubleshooting_basics/Generic_Troubleshooting_Guide.svg" /&gt;&lt;/p&gt;</content><category term="Software Engineering"></category><category term="Best Practice"></category><category term="System Engineering"></category></entry><entry><title>Is GitHub Copilot a Threat to Developers? (Spoiler: It's Not)</title><link href="https://aveuiller.github.io/is_copilot_a_threat.html" rel="alternate"></link><published>2021-07-26T00:00:00+02:00</published><updated>2021-07-26T00:00:00+02:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2021-07-26:/is_copilot_a_threat.html</id><summary type="html">&lt;p&gt;GitHub Copilot provides potentially scary possibilities but can be a fine addition to the developers' tools.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hacker Noon: &lt;a href="https://hackernoon.com/will-github-copilot-replace-developers-spoiler-no-jh5r35fl"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Medium: &lt;a href="https://medium.com/geekculture/towards-abstraction-of-computer-science-cc68e4c30654"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dev.to: &lt;a href="https://dev.to/aveuiller/is-github-copilot-a-threat-to-developers-spoiler-it-s-not-5ee1"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img alt="Illustration" src="/images/posts/2021-07-26_abstraction_computer_science/copilot.png" /&gt;&lt;/p&gt;
&lt;p&gt;Computer science provides more and more abstraction layers with time, to the point that we don’t really know what’s running underneath.&lt;/p&gt;
&lt;p&gt;GitHub recently released &lt;a href="https://copilot.github.com/"&gt;Copilot&lt;/a&gt; in technical preview.
Copilot is an AI-based assistant that will automatically generate code from the context of your application, with the aim of easing software development.&lt;/p&gt;
&lt;p&gt;A question that arises with simplification and automation in computer science is generally &lt;em&gt;“will the developer still be needed?”&lt;/em&gt;.
And every time it happened, the answer was &lt;em&gt;“yes”&lt;/em&gt;.
Abstraction in computer science reduces the cost of entering the domain, thus attracts a larger population.
But it also usually creates opportunities with new technologies, practices, and more complex systems.&lt;/p&gt;
&lt;p&gt;This short article aims to summarize the evolution of abstraction layers in computer science to help understand the impact that GitHub Copilot may have.&lt;/p&gt;
&lt;h3&gt;A Bit of History&lt;/h3&gt;
&lt;p&gt;The example that seems the oldest and most sensible would be the &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;Turing Machine&lt;/a&gt;.
It was created with the specific idea to break the cryptographic code &lt;a href="https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Enigma"&gt;Enigma&lt;/a&gt;,
but the concepts behind it are now considered as a &lt;a href="https://en.wikipedia.org/wiki/Turing_completeness"&gt;ground reference of algorithmic capabilities&lt;/a&gt; for programming languages.&lt;/p&gt;
&lt;p&gt;Since then, we created processors, able to take a random set of instructions and process them generically.
Those instructions were originally described using &lt;a href="https://en.wikipedia.org/wiki/Machine_code"&gt;machine code&lt;/a&gt;, which is hard to manipulate for a human.
This is why programming languages of higher abstraction appeared.&lt;/p&gt;
&lt;p&gt;Generally speaking, we can describe the goal of a programming language as to &lt;em&gt;write human-readable specifications that can be converted into machine code.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Evolution of Programming&lt;/h3&gt;
&lt;p&gt;Programming languages evolved from &lt;em&gt;low (abstraction) level&lt;/em&gt; languages to &lt;em&gt;higher levels&lt;/em&gt;, each iteration adding automation and helpers to the common operations.
Those abstractions can take many forms, from &lt;a href="https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29"&gt;garbage collection&lt;/a&gt; to libraries
&lt;a href="https://docs.python.org/3/library/csv.html?highlight=read#examples"&gt;reading CSV files&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These evolutions enable developers to add more complexity to their software, as there is less complexity on the technical parts.
They can in turn abstract away even more for others. Thanks to this, taking on programming is becoming easier with time.
We even see some programming languages using almost natural languages (e.g. &lt;a href="https://docs.behat.org/en/v2.5/guides/1.gherkin.html"&gt;Gherkin&lt;/a&gt;)
and requiring next to zero computer science knowledge to write.&lt;/p&gt;
&lt;p&gt;This does not mean that people will stop using languages with lower abstraction levels.
On the contrary, even if they are generally harder to master and more verbose, they can be way more flexible for some use cases,
memory management for instance.&lt;/p&gt;
&lt;p&gt;We can also note the appearance of Machine Learning, creating dynamic processes over data that would have been tedious to analyse, either by hand or through specific code.
This enables writing potentially complex behaviours with a few lines of code in some cases. 
Even then, there is some &lt;a href="https://cloud.google.com/automl"&gt;automation&lt;/a&gt; of it to the point where you only have to provide data to get working results.&lt;/p&gt;
&lt;p&gt;That being said, having a minimum of knowledge on the layers underneath remains very important to really comprehend the code execution.
Otherwise, it would be difficult to adapt to issues that may, and will, arise.&lt;/p&gt;
&lt;h3&gt;Evolution of Infrastructure&lt;/h3&gt;
&lt;p&gt;We talked about code, but the same kind of evolution happened on the servers too.
At first, companies were starting on personal computers until they could get a hold of some space in a datacenter, renting one or more &lt;em&gt;baremetal&lt;/em&gt; servers.
This was costly and required technical knowledge for both the installation and deployment processes.&lt;/p&gt;
&lt;p&gt;The virtualization technology evolved to the point where you can now rent a &lt;em&gt;virtual machine&lt;/em&gt; hosted on a physical host anywhere through 
&lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-are-cloud-providers"&gt;cloud providers&lt;/a&gt;. 
This tremendously reduces the costs and enhances accessibility, but still requires technical knowledge to administrate correctly the machine on top of the deployments pipelines.&lt;/p&gt;
&lt;p&gt;Since more recently, the technology of &lt;em&gt;containerization&lt;/em&gt; is heavily used with powerful tools like &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; and 
&lt;a href="https://aveuiller.medium.com/kubernetes-apprentice-cookbook-90d8c11ccfc3"&gt;Kubernetes&lt;/a&gt;. 
Those tools can help you deploy applications on a VM without necessarily mastering the system underneath*.
In the same fashion as machine learning, there are projects aiming to further &lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview"&gt;reduce the knowledge required&lt;/a&gt;
to use such technologies.&lt;/p&gt;
&lt;p&gt;On top of that, a new abstraction layer called &lt;em&gt;serverless&lt;/em&gt; is emerging.
This enables you to directly execute code on a remote container, without having to take care of the hosting problematics at all.
While this is currently mostly used for stateless operations, there are no doubts about the growth of use cases and tools around this ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;*Note: Those tools may be better used with knowledge on the underlying layer, but you can redirect your learning to a more practical proficiency.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;A Glimpse on The Future of Copilot&lt;/h3&gt;
&lt;p&gt;GitHub Copilot is a pretty interesting advancement in software engineering, even if not completely on point at the moment.
According to the FAQ, the system gets the generation right around 50% of the time, and it is recommended to stay wary of the generated code.&lt;/p&gt;
&lt;p&gt;If we look a bit differently on GitHub Copilot, we can consider this as a new programming language.
The input of this language is the documentation and function name and those are &lt;a href="https://en.wiktionary.org/wiki/transcompiler"&gt;transpiled&lt;/a&gt; into source code for another programming language.&lt;/p&gt;
&lt;p&gt;The generation is today limited to methods but we can imagine that this will evolve to classes or even packages.
We can also imagine the generation becoming almost flawless for well-known use cases.&lt;/p&gt;
&lt;p&gt;Even if everything is one day working on a larger scale and flawlessly, the generated code will still require developers to handle the direction it takes,
and connect the even bigger pictures altogether.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In regard to what we just saw, it makes no doubt that GitHub Copilot will further abstract development and ease the creation of software.
This breakthrough has the power to move the development industry, as other abstraction layers did before.&lt;/p&gt;
&lt;p&gt;Having said that, developers will probably never lack opportunities, as the scope of the projects will get bigger thanks to this new abstraction layer.
The day to day work done as a developer may change, but developers will still be required for a long time.&lt;/p&gt;
&lt;p&gt;As a matter of comparison, we already saw system administration positions evolve to SRE, still working on providing reliable hardware, but on a scale never seen before.
In the same way, developers may evolve to something close, adapting to the new tools.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Sarra Habchi for the review&lt;/em&gt;&lt;/p&gt;</content><category term="Around Computer Science"></category><category term="showerthoughs"></category></entry><entry><title>Apache Kafka: Apprentice Cookbook</title><link href="https://aveuiller.github.io/kafka_apprentice_cookbook.html" rel="alternate"></link><published>2021-06-29T00:00:00+02:00</published><updated>2021-06-29T00:00:00+02:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2021-06-29:/kafka_apprentice_cookbook.html</id><summary type="html">&lt;p&gt;Apache Kafka is a distributed event streaming platform built over strong concepts. Let's dive into the possibilities it offers.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hacker Noon: &lt;a href="https://hackernoon.com/the-apprentices-guide-to-apache-kafka-n31w35ef"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Medium: &lt;a href="https://aveuiller.medium.com/557439273cee"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dev.to: &lt;a href="https://dev.to/aveuiller/apache-kafka-apprentice-cookbook-26m"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img alt="Apache Kafka Logo" src="/images/posts/2021-06-01_Kafka-Apprentice-Cookbook/kafka_logo.png" /&gt;&lt;/p&gt;
&lt;p&gt;Apache Kafka is a distributed event streaming platform built over strong concepts.
Let’s dive into the possibilities it offers.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; is a distributed event streaming platform built with an emphasis on reliability,
performance, and customization. Kafka can send and receive messages in a &lt;a href="https://aws.amazon.com/pub-sub-messaging/"&gt;publish-subscribe&lt;/a&gt; fashion.
To achieve this, the ecosystem relies on few but strong basic concepts,
which enable the community to build many features solving &lt;a href="https://kafka.apache.org/uses"&gt;numerous use cases&lt;/a&gt;, for instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Processing messages as an &lt;a href="https://www.confluent.io/blog/apache-kafka-vs-enterprise-service-bus-esb-friends-enemies-or-frenemies/"&gt;Enterprise Service Bus&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Tracking Activity, metrics, and telemetries.&lt;/li&gt;
&lt;li&gt;Processing Streams.&lt;/li&gt;
&lt;li&gt;Supporting &lt;a href="https://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/"&gt;Event sourcing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Storing logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This article will see the concepts backing up Kafka and the different tools available to handle data streams.&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;The behaviour of Kafka is pretty simple: &lt;strong&gt;Producers&lt;/strong&gt; push &lt;em&gt;Messages&lt;/em&gt; into a particular &lt;em&gt;Topic&lt;/em&gt;,
and &lt;strong&gt;Consumers&lt;/strong&gt; subscribe to this &lt;em&gt;Topic&lt;/em&gt; to fetch and process the &lt;em&gt;Messages&lt;/em&gt;.
Let’s see how it is achieved by this technology.&lt;/p&gt;
&lt;h3&gt;Infrastructure side&lt;/h3&gt;
&lt;p&gt;Independently of the use, the following components will be deployed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more &lt;strong&gt;Producers&lt;/strong&gt; sending messages to the brokers.&lt;/li&gt;
&lt;li&gt;One or more Kafka &lt;strong&gt;Brokers&lt;/strong&gt;, the actual messaging server handling communication between producers and consumers.&lt;/li&gt;
&lt;li&gt;One or more &lt;strong&gt;Consumers&lt;/strong&gt; fetching and processing messages, in clusters named &lt;strong&gt;Consumer Groups&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;One or more &lt;a href="https://zookeeper.apache.org/"&gt;&lt;strong&gt;Zookeeper&lt;/strong&gt;&lt;/a&gt; instances managing the brokers.&lt;/li&gt;
&lt;li&gt;(Optionally) One or more &lt;strong&gt;Registry&lt;/strong&gt; instances uniformizing message schema.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a scalable distributed system, Kafka is heavily relying on the concept of &lt;em&gt;clusters&lt;/em&gt;.
As a result, on typical production deployment, there will likely be multiple instances of each component.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Consumer Group&lt;/strong&gt; is a cluster of the same consumer application.
This concept is heavily used by Kafka to balance the load on the applicative side of things.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kafka Architecture" src="/images/posts/2021-06-01_Kafka-Apprentice-Cookbook/kafka_architecture.svg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: The dependency on Zookeeper will be removed soon, Cf.&lt;/em&gt; &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum"&gt;&lt;em&gt;KIP-500&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kafka.apache.org/documentation/#majordesignelements"&gt;Design &amp;amp; Implementation Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hackernoon.com/kafka-basics-and-core-concepts-explained-dd1434dv"&gt;Kafka Basics and Core Concepts: Explained  — Aritra Das&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Applicative side&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;Message&lt;/strong&gt; in Kafka is a &lt;code&gt;key-value&lt;/code&gt; pair.
Those elements can be anything from an integer to a &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protobuf message&lt;/a&gt;,
provided the right serializer and deserializer.&lt;/p&gt;
&lt;p&gt;The message is sent to a &lt;strong&gt;Topic&lt;/strong&gt;, which will store it as a &lt;strong&gt;Log&lt;/strong&gt;.
The topic should be a collection of logs semantically related, but without a particular structure imposed.
A topic can either keep every message as a new log entry or only keep the last value for each key
(a.k.a. &lt;a href="https://docs.confluent.io/platform/current/kafka/design.html#log-compaction"&gt;Compacted log&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;To take advantage of the multiple brokers, topics are &lt;a href="https://en.wikipedia.org/wiki/Shard_%28database_architecture%29"&gt;sharded&lt;/a&gt;
into &lt;strong&gt;Partitions&lt;/strong&gt; by default.
Kafka will assign any received message to one partition depending on its key,
or using &lt;a href="https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner"&gt;a partitioner algorithm&lt;/a&gt; otherwise,
which results in a random assignment from the developer's point of view.
Each partition has a &lt;strong&gt;Leader&lt;/strong&gt; responsible for all I/O operations, and &lt;strong&gt;Followers&lt;/strong&gt; replicating the data.
A follower will take over the leader role in case of an issue with the current one.&lt;/p&gt;
&lt;p&gt;The partition holds the received data in order, increasing an &lt;strong&gt;offset&lt;/strong&gt; integer for each message.
However, there is no order guarantee between two partitions.
So for order-dependent data, one must ensure that they end up in the same partition by using the same key.&lt;/p&gt;
&lt;p&gt;Each partition is assigned to a specific consumer from the consumer group.
This consumer is the only one fetching messages from this partition.
In case of shutdown of one customer, the brokers will &lt;a href="https://medium.com/streamthoughts/understanding-kafka-partition-assignment-strategies-and-how-to-write-your-own-custom-assignor-ebeda1fc06f3"&gt;reassign partitions&lt;/a&gt;
among the customers.&lt;/p&gt;
&lt;p&gt;Being an asynchronous system, it can be hard and impactful on the performances to have every message delivered exactly one time to the consumer.
To mitigate this, Kafka provides &lt;a href="https://kafka.apache.org/documentation/#semantics"&gt;different levels of guarantee&lt;/a&gt;
on the number of times a message will be processed (&lt;em&gt;i.e.&lt;/em&gt; at most once, at least once, exactly once).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7"&gt;Log Compacted Topics in Apache Kafka — Seyed Morteza Mousavi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Vo6Mv5YPOJU&amp;amp;list=PLa7VYi0yPIH0KbnJQcMv5N9iW8HkZHztH&amp;amp;index=5"&gt;(Youtube) Apache Kafka 101: Replication — Confluent&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication"&gt;Replication Design Doc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@andy.bryant/processing-guarantees-in-kafka-12dd2e30be0e"&gt;Processing Guarantees in Details — Andy Briant&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Schema and Registry&lt;/h3&gt;
&lt;p&gt;Messages are serialized when quitting a producer and deserialized when handled by the consumer.
To ensure compatibility, both must be using the same data definition.
Ensuring this can be hard considering the application evolution.
As a result, when dealing with a production system, it is recommended to use a schema to explicit a contract on the data structure.&lt;/p&gt;
&lt;p&gt;To do this, Kafka provides a &lt;strong&gt;Registry&lt;/strong&gt; server, storing and binding schema to topics.
Historically only &lt;a href="https://avro.apache.org/docs/current/"&gt;Avro&lt;/a&gt; was available, but the registry is now modular and can also handle
&lt;a href="https://json-schema.org/"&gt;JSON&lt;/a&gt; and &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protobuf&lt;/a&gt; out of the box.&lt;/p&gt;
&lt;p&gt;Once a producer sent a schema describing the data handled by its topic to the registry, other parties
(&lt;em&gt;i.e.&lt;/em&gt; brokers and consumers) will fetch this schema on the registry to validate and deserialize the data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/schema-registry/index.html"&gt;Schema Registry Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aseigneurin.github.io/2018/08/02/kafka-tutorial-4-avro-and-schema-registry.html"&gt;Kafka tutorial #4-Avro and the Schema Registry— Alexis Seigneurin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#serializer-and-formatter"&gt;Serializer-Deserializer for Schema&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Integrations&lt;/h2&gt;
&lt;p&gt;Kafka provides multiple ways of connecting to the brokers,
and each can be more useful than the others depending on the needs.
As a result, even if a library is an abstraction layer above another, it is not necessarily better for every use case.&lt;/p&gt;
&lt;h3&gt;Kafka library&lt;/h3&gt;
&lt;p&gt;There are client libraries available in &lt;a href="https://docs.confluent.io/platform/current/clients/index.html"&gt;numerous languages&lt;/a&gt;
which help develop a producer and consumer easily.
We will use Java for the example below, but the concept remains identical for other languages.&lt;/p&gt;
&lt;p&gt;The producer concept is to publish messages at any moment, so the code is pretty simple.&lt;/p&gt;
&lt;p&gt;```java
public class Main {
  public static void main(String[] args) throws Exception {
    // Configure your producer
    Properties producerProperties = new Properties();
    producerProperties.put("bootstrap.servers", "localhost:29092");
    producerProperties.put("acks", "all");
    producerProperties.put("retries", 0);
    producerProperties.put("linger.ms", 1);
    producerProperties.put("key.serializer", "org.apache.kafka.common.serialization.LongSerializer");
    producerProperties.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
    producerProperties.put("schema.registry.url", "http://localhost:8081");&lt;/p&gt;
&lt;p&gt;// Initialize a producer
    Producer&lt;Long, AvroHelloMessage&gt; producer = new KafkaProducer&amp;lt;&amp;gt;(producerProperties);&lt;/p&gt;
&lt;p&gt;// Use it whenever you need
    producer.send(new AvroHelloMessage(1L, "this is a message", 2.4f, 1));
  }
}
```&lt;/p&gt;
&lt;p&gt;The code is a bit more complex on the consumer part since the consumption loop needs to be created manually.
On the other hand, this gives more control over its behaviour.
The consumer state is automatically handled by the Kafka library.
As a result, restarting the worker will start at the most recent offset he encountered.&lt;/p&gt;
&lt;p&gt;```java
public class Main {
    public static Properties configureConsumer() {
        Properties consumerProperties = new Properties();&lt;/p&gt;
&lt;p&gt;consumerProperties.put("bootstrap.servers", "localhost:29092");
        consumerProperties.put("group.id", "HelloConsumer");
        consumerProperties.put("key.deserializer", "org.apache.kafka.common.serialization.LongDeserializer");
        consumerProperties.put("value.deserializer", "io.confluent.kafka.serializers.KafkaAvroDeserializer");
        consumerProperties.put("schema.registry.url", "http://localhost:8081");
        // Configure Avro deserializer to convert the received data to a SpecificRecord (i.e. AvroHelloMessage)
        // instead of a GenericRecord (i.e. schema + array of deserialized data).
        consumerProperties.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);&lt;/p&gt;
&lt;p&gt;return consumerProperties;
    }&lt;/p&gt;
&lt;p&gt;public static void main(String[] args) throws Exception {
        // Initialize a consumer
        final Consumer&lt;Long, AvroHelloMessage&gt; consumer = new KafkaConsumer&amp;lt;&amp;gt;(configureConsumer());&lt;/p&gt;
&lt;p&gt;// Chose the topics you will be polling from.
        // You can subscribe to all topics matching a Regex.
        consumer.subscribe(Pattern.compile("hello_topic_avro"));&lt;/p&gt;
&lt;p&gt;// Poll will return all messages from the current consumer offset
        final AtomicBoolean shouldStop = new AtomicBoolean(false);
        Thread consumerThread = new Thread(() -&amp;gt; {
            final Duration timeout = Duration.ofSeconds(5);&lt;/p&gt;
&lt;p&gt;while (!shouldStop) {
                for (ConsumerRecord&lt;Long, AvroHelloMessage&gt; record : consumer.poll(timeout)) {
                    // Use your record
                    AvroHelloMessage value = record.value();
                }
                // Be kind to the broker while polling
                Thread.sleep(5);
            }&lt;/p&gt;
&lt;p&gt;consumer.close(timeout);
        });&lt;/p&gt;
&lt;p&gt;// Start consuming &amp;amp;&amp;amp; do other things
        consumerThread.start();
        // [...]&lt;/p&gt;
&lt;p&gt;// End consumption from customer
        shouldStop.set(true);
        consumerThread.join();
    }
}
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/clients/index.html"&gt;Available Libraries&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html"&gt;Producer Configuration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html"&gt;Consumer Configuration&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Kafka Streams&lt;/h3&gt;
&lt;p&gt;Kafka Streams is built on top of the consumer library.
It continuously reads from a topic and processes the messages with code declared with a functional DSL.&lt;/p&gt;
&lt;p&gt;During the processing, transitional data can be kept in structures called &lt;a href="https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/kstream/KStream.html"&gt;KStream&lt;/a&gt;
and &lt;a href="https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/kstream/KTable.html"&gt;KTable&lt;/a&gt;,
which are stored into topics. The former is equivalent to a standard topic, and the latter to a compacted topic.
Using these data stores will enable automatic tracking of the worker state by Kafka, helping to get back on track in case of restart.&lt;/p&gt;
&lt;p&gt;The following code sample is extracted from the &lt;a href="https://kafka.apache.org/28/documentation/streams/tutorial"&gt;tutorial provided by Apache&lt;/a&gt;.
The code connects to a topic named &lt;code&gt;streams-plaintext-input&lt;/code&gt; containing strings values, without necessarily providing keys.
The few lines configuring the &lt;code&gt;StreamsBuilder&lt;/code&gt; will:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Transform each message to lowercase.&lt;/li&gt;
&lt;li&gt;Split the result using whitespaces as a delimiter.&lt;/li&gt;
&lt;li&gt;Group previous tokens by value.&lt;/li&gt;
&lt;li&gt;Count the number of tokens for each group and save the changes to a KTable named &lt;code&gt;counts-store&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Stream the changes in this Ktable to send the values in a KStream named &lt;code&gt;streams-wordcount-output&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;```java
public class Main {
  public static void main(String[] args) throws Exception {
    Properties props = new Properties();
    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");
    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:29092");
    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());&lt;/p&gt;
&lt;p&gt;final StreamsBuilder builder = new StreamsBuilder();&lt;/p&gt;
&lt;p&gt;builder.&lt;String, String&gt;stream("streams-plaintext-input")
            .flatMapValues(value -&amp;gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\W+")))
            .groupBy((key, value) -&amp;gt; value)
            .count(Materialized.&lt;String, Long, KeyValueStore\&lt;Bytes, byte[]&gt;&gt;as("counts-store"))
            .toStream()
            .to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));&lt;/p&gt;
&lt;p&gt;final Topology topology = builder.build();
    final KafkaStreams streams = new KafkaStreams(topology, props);
    final CountDownLatch latch = new CountDownLatch(1);&lt;/p&gt;
&lt;p&gt;// attach shutdown handler to catch control-c
    Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
      @Override
      public void run() {
        streams.close();
        latch.countDown();
      }
    });&lt;/p&gt;
&lt;p&gt;// The consumer loop is handled by the library
    streams.start();
    latch.await();
  }
}
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/streams/concepts.html"&gt;Kafka Streams Concepts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/streams/developer-guide/write-streams.html"&gt;Developer Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@andy.bryant/kafka-streams-work-allocation-4f31c24753cc"&gt;Kafka Stream Work Allocation — Andy Briant&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Kafka Connect&lt;/h3&gt;
&lt;p&gt;Kafka Connect provides a way of transforming and synchronizing data between almost any technology with the use of &lt;strong&gt;Connectors&lt;/strong&gt;.
Confluent is hosting a &lt;a href="https://www.confluent.io/hub/"&gt;Hub&lt;/a&gt;, on which users can share connectors for various technologies.
This means that integrating a Kafka Connect pipeline is most of the time only a matter of configuration, without code required.
A single connector can even handle both connection sides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Populate a topic with data from any system: &lt;em&gt;i.e.&lt;/em&gt; a &lt;strong&gt;Source&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Send data from a topic to any system: &lt;em&gt;i.e.&lt;/em&gt; a &lt;strong&gt;Sink&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The source will read data from CSV files in the following schema then publish them into a topic.
Concurrently, the sink will poll from the topic and insert the messages into a MongoDB database.
Each connector can run in the same or a distinct worker, and workers can be grouped into a cluster for scalability.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kafka Connect Example" src="/images/posts/2021-06-01_Kafka-Apprentice-Cookbook/kafka_connect.png" /&gt;&lt;/p&gt;
&lt;p&gt;The connector instance is created through a configuration specific to the library.
The file below is a configuration of the &lt;a href="https://www.confluent.io/hub/mongodb/kafka-connect-mongodb"&gt;MongoDB connector&lt;/a&gt;.
It asks to fetch all messages from the topic &lt;code&gt;mongo-source&lt;/code&gt; to insert them into the collection &lt;code&gt;sink&lt;/code&gt; of the database named &lt;code&gt;kafka-connect&lt;/code&gt;.
The credentials are provided from an external file, which is a feature of Kafka Connect to &lt;a href="https://docs.confluent.io/platform/current/connect/security.html#externalizing-secrets"&gt;protect secrets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;json
{
  "name": "mongo-sink",
  "config": {
    "topics": "mongo-source",
    "tasks.max": "1",
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "connection.uri": "mongodb://${file:/auth.properties:username}:${file:/auth.properties:password}@mongo:27017",
    "database": "kafka_connect",
    "collection": "sink",
    "max.num.retries": "1",
    "retries.defer.timeout": "5000",
    "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy",
    "post.processor.chain": "com.mongodb.kafka.connect.sink.processor.DocumentIdAdder",
    "delete.on.null.values": "false",
    "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy"
  }
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Once the configuration complete, registering the connector is as easy as an HTTP call on the running &lt;a href="https://docs.confluent.io/home/connect/userguide.html#configuring-and-running-workers"&gt;Kafka Connect instance&lt;/a&gt;.
Afterwards, the service will automatically watch the data without further work required.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;shell
$ curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" \
  http://localhost:8083/connectors -d @sink-conf.json&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/connect/userguide.html#connect-userguide"&gt;Getting Started Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/connect/references/restapi.html"&gt;Connector Instance API Reference&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLa7VYi0yPIH1MB2n2w8pMZguffCDu2L4Y"&gt;(Youtube) Tutorials Playlist — Confluent&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;KSQL Database&lt;/h3&gt;
&lt;p&gt;Ksql is somehow equivalent to Kafka Streams, except that every transformation is declared in an SQL-like language.
The server is connected to the brokers and can create &lt;strong&gt;Streams&lt;/strong&gt; or &lt;strong&gt;Tables&lt;/strong&gt; from topics.
Those two concepts behave in the same way as a KStream or KTable from Kafka Streams (&lt;em&gt;i.e.&lt;/em&gt; respectively a topic and a compacted topic).&lt;/p&gt;
&lt;p&gt;There are three types of query in the language definition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Persistent Query&lt;/strong&gt; (&lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;CREATE TABLE &amp;lt;name&amp;gt; WITH (...)&lt;/code&gt;): Creates a new stream or table that will be automatically updated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull Query&lt;/strong&gt; (&lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;SELECT * FROM &amp;lt;table|stream&amp;gt; WHERE ID = 1&lt;/code&gt;): Behaves similarly to a standard DBMS. Fetches data as an instant snapshot and closes the connection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Push Query&lt;/strong&gt; (&lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;SELECT * FROM &amp;lt;table|stream&amp;gt; EMIT CHANGES&lt;/code&gt;): Requests a persistent connection to the server, asynchronously pushing updated values.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The database can be used to browse the brokers' content. Topics can be discovered through the command &lt;code&gt;list topics&lt;/code&gt;, and their content displayed using &lt;code&gt;print &amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;```sql
ksql&amp;gt; list topics;
 Kafka Topic      | Partitions | Partition Replicas&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;hello_topic_json | 1          | 1&lt;/h2&gt;
&lt;p&gt;ksql&amp;gt; print 'hello_topic_json' from beginning;
Key format: KAFKA_BIGINT or KAFKA_DOUBLE or KAFKA_STRING
Value format: JSON or KAFKA_STRING
rowtime: 2021/05/25 08:44:20.922 Z, key: 1, value: {"user_id":1,"message":"this is a message","value":2.4,"version":1}
rowtime: 2021/05/25 08:44:20.967 Z, key: 1, value: {"user_id":1,"message":"this is another message","value":2.4,"version":2}
rowtime: 2021/05/25 08:44:20.970 Z, key: 2, value: {"user_id":2,"message":"this is another message","value":2.6,"version":1}
```&lt;/p&gt;
&lt;p&gt;The syntax to create and query a stream, or a table is very close to SQL.&lt;/p&gt;
&lt;p&gt;```sql
-- Let's create a table from the previous topic
ksql&amp;gt; CREATE TABLE messages (user_id BIGINT PRIMARY KEY, message VARCHAR) &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;WITH (KAFKA_TOPIC = 'hello_topic_json', VALUE_FORMAT='JSON');&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- We can see the list and details of each table
ksql&amp;gt; list tables;
 Table Name | Kafka Topic      | Key Format | Value Format | Windowed&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;MESSAGES   | hello_topic_json | KAFKA      | JSON         | false&lt;/h2&gt;
&lt;p&gt;ksql&amp;gt; describe messages;
Name                 : MESSAGES
 Field   | Type&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;USER_ID | BIGINT           (primary key)
 MESSAGE | VARCHAR(STRING)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For runtime statistics and query details run: DESCRIBE EXTENDED &lt;Stream,Table&gt;;&lt;/p&gt;
&lt;p&gt;-- Appart from some additions to the language, the queries are almost declared in standard SQL. 
ksql&amp;gt; select * from messages EMIT CHANGES;
+--------+------------------------+
|USER_ID |MESSAGE                 |
+--------+------------------------+
|1       |this is another message |
|2       |this is another message |
```&lt;/p&gt;
&lt;p&gt;Kafka recommends using a &lt;a href="https://www.confluent.io/blog/deep-dive-ksql-deployment-options/"&gt;headless ksqlDB server&lt;/a&gt;
for production, with a file declaring all streams and tables to create.
This avoids any modification to the definitions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: ksqlDB servers can be grouped in a cluster like any other consumer.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further Reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.confluent.io/platform/current/streams-ksql.html"&gt;Official Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.ksqldb.io/en/latest/concepts/queries/"&gt;KSQL Query Types In Details&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLa7VYi0yPIH2eX8q3mPpZAn3qCS1eDX8W"&gt;(Youtube) Tutorials Playlist — Confluent&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This article gives a broad view of the Kafka ecosystem and possibilities, which are numerous.
This article only scratches the surface of each subject.
But worry not, as they are all well documented by Apache, Confluent, and fellow developers.
Here are a few supplementary resources to dig further into Kafka:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLa7VYi0yPIH0KbnJQcMv5N9iW8HkZHztH"&gt;(Youtube) Kafka Tutorials - &lt;em&gt;Confluent&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kafka-tutorials.confluent.io/"&gt;Kafka Tutorials in Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.confluent.io/blog/5-things-every-kafka-developer-should-know/"&gt;Top 5 Things Every Apache Kafka Developer Should Know — Bill Bejeck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.confluent.io/platform/current/app-development/kafkacat-usage.html"&gt;Kafkacat user Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.confluent.io/blog/troubleshooting-ksql-part-2"&gt;Troubleshooting KSQL Part 2: What’s Happening Under the Covers? — Robin Moffatt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ssudan16.medium.com/kafka-internals-47e594e3f006"&gt;Apache Kafka Internals — sudan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;The complete experimental code is available on my &lt;a href="https://github.com/aveuiller/frameworks-bootstrap/tree/master/Kafka"&gt;GitHub repository&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Sarra Habchi, and Dimitri Delabroye for the reviews&lt;/em&gt;&lt;/p&gt;</content><category term="Software Engineering"></category><category term="Cheat sheet"></category><category term="Kafka"></category></entry><entry><title>Kubernetes: Apprentice Cookbook</title><link href="https://aveuiller.github.io/kubernetes_apprentice_cookbook.html" rel="alternate"></link><published>2021-05-10T00:00:00+02:00</published><updated>2021-05-10T00:00:00+02:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2021-05-10:/kubernetes_apprentice_cookbook.html</id><summary type="html">&lt;p&gt;Kubernetes big picture and common use&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hacker Noon: &lt;a href="https://hackernoon.com/the-apprentices-guide-to-kubernetes-qp3k3443"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Medium: &lt;a href="https://medium.com/@aveuiller/kubernetes-apprentice-cookbook-90d8c11ccfc3"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dev.to: &lt;a href="https://dev.to/aveuiller/kubernetes-apprentice-cookbook-4j6h"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img alt="Kubernetes Logo" src="/images/posts/2021-05-10_Kubernetes-Apprentice-Cookbook/kube_logo.png" /&gt;&lt;/p&gt;
&lt;p&gt;You probably already heard of &lt;strong&gt;Kubernetes&lt;/strong&gt;, a powerful &lt;a href="https://www.redhat.com/en/topics/automation/what-is-orchestration"&gt;orchestrator&lt;/a&gt;
that will ease deployment and automatically manage your applications on a set of machines, called a &lt;em&gt;Cluster&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With great power comes great complexity, &lt;a href="https://www.theregister.com/2021/02/25/google_kubernetes_autopilot/"&gt;even in the eyes of Google&lt;/a&gt;.
Thus, learning Kubernetes is oftentimes considered as cumbersome and complex, namely because of the number of new concepts you have to learn.
On the other hand, those very same concepts can be found in other orchestrators.
As a result, mastering them will ease your onboarding on other orchestrators, such as &lt;a href="https://docs.docker.com/engine/swarm/"&gt;Docker Swarm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The aim of this article is to explain the most used concepts of Kubernetes relying on basic system administration concepts,
then use some of these to deploy a simple web server and showcase the interactions between the different resources.
Lastly, I will lay out the usual CLI interactions while working with Kubernetes.&lt;/p&gt;
&lt;p&gt;This article mainly focuses on the developer side of a Kubernetes cluster, but I will leave some resources about cluster administration at the end.&lt;/p&gt;
&lt;h2&gt;Terminology and concepts&lt;/h2&gt;
&lt;h3&gt;Architecture&lt;/h3&gt;
&lt;p&gt;The Kubernetes realm is the &lt;strong&gt;cluster&lt;/strong&gt;, everything needed is contained within this cluster.
Inside it, you will find two types of nodes:
the &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#control-plane-components"&gt;Control Plane&lt;/a&gt; 
and the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/"&gt;Worker Nodes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;control plane&lt;/strong&gt; is a centralized set of processes that manages the cluster resources, load balance, health, and more.
A Kubernetes cluster usually has multiple controller nodes for availability and load balancing purposes.
As a developer, you will most likely interact through the API gateway for interactions.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;worker node&lt;/strong&gt; is any kind of host running a local Kubernetes agent &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"&gt;Kubelet&lt;/a&gt;
and a communication process &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/"&gt;Kube-Proxy&lt;/a&gt;.
The former handles the operations commanded by the &lt;strong&gt;control plane&lt;/strong&gt; on the local container runtime (&lt;em&gt;e.g.&lt;/em&gt; docker),
while the latter redirects connectivity to the right pods.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kubernetes Architecture" src="/images/posts/2021-05-10_Kubernetes-Apprentice-Cookbook/kube_components.svg" /&gt;&lt;/p&gt;
&lt;h3&gt;Namespaces&lt;/h3&gt;
&lt;p&gt;After some time, a Kubernetes cluster may become huge and heavily used.
In order to keep things well organized, Kubernetes created the concept of &lt;strong&gt;Namespace&lt;/strong&gt;.
A namespace is basically a virtual cluster inside the actual cluster.&lt;/p&gt;
&lt;p&gt;Most of the resources will be contained inside a namespace, thus unaware of resources from other namespaces.
Only a few kinds of resources are completely agnostic of namespaces, and they define computational power or storage sources (&lt;em&gt;i.e.&lt;/em&gt; Nodes and PersistentVolumes).
However, access to those can be limited by namespace using &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/"&gt;Quotas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Namespace-aware resources will always be contained in a namespace as Kubernetes creates and uses a namespace named &lt;em&gt;default&lt;/em&gt; if nothing is specified.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Namespace Organization" src="/images/posts/2021-05-10_Kubernetes-Apprentice-Cookbook/kube_namespace.svg" /&gt;&lt;/p&gt;
&lt;p&gt;There is no silver bullet on the way to use namespaces, as it widely depends on your organization and needs.
However, we can note some usual namespaces usages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Divide the cluster by team or project, to avoid naming conflict and help repartition of resources.&lt;/li&gt;
&lt;li&gt;Divide the cluster by environment (&lt;em&gt;i.e.&lt;/em&gt; dev, staging, prod), to keep a consistent architecture.&lt;/li&gt;
&lt;li&gt;Deploy with more granularity (&lt;em&gt;e.g.&lt;/em&gt; &lt;a href="https://martinfowler.com/bliki/BlueGreenDeployment.html"&gt;blue/green deployment&lt;/a&gt;), to quickly fall back on an untouched working environment in case of issue.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"&gt;Namespace Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/"&gt;Manage The Cluster Namespaces&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Glossary&lt;/h3&gt;
&lt;p&gt;Kubernetes did a great work of remaining agnostic of any technology in their design.
This means two things:  &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/"&gt;handle multiple technologies under the hood&lt;/a&gt;
and there is a whole new terminology to learn.&lt;/p&gt;
&lt;p&gt;Fortunately, these concepts are pretty straightforward and can most of the time be compared to a unit element of classic system infrastructure.
The table below will summarize the binding of the most basic concepts.
The comparison might not be a hundred per cent accurate but rather here to help understand the need behind each concept.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Abstraction Layer&lt;/th&gt;
&lt;th&gt;Physical Layer&lt;/th&gt;
&lt;th&gt;Uses Namespace&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;Pod&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Container&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A Pod is the minimal work unit of Kubernetes, it is generally equivalent to one applicative container but it can be composed of multiple ones.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"&gt;Replicaset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Load Balancing&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A ReplicaSet keeps track of and maintain the amount of instances expected and running for a given pod.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A Deployment keeps track of and maintain the required configuration for a pod and replicaset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"&gt;StatefulSet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A StatefulSet is a Deployment with insurance on the start order and volume binding, to keep state consistent in time.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/"&gt;Node&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Host&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;A Node can be a physical or virtual machine that is ready to host pods.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Network&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A Service will define an entrypoint to a set of pods semantically tied together.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reverse Proxy&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;An Ingress publishes Services outside the Cluster.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/#design"&gt;Cluster&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Datacenter&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;A Cluster is the set of available nodes, including the Kubernetes controllers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"&gt;Namespace&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;➖&lt;/td&gt;
&lt;td&gt;A Namespace defines an isolated pseudo cluster in the current cluster.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/"&gt;StorageClass&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Disk&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;A StorageClass configures filesystems sources that can be used to dynamically create PersistentVolumes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;PersistentVolume&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Disk Partition&lt;/td&gt;
&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;A PersistentVolume describe any kind of filesystem ready to be mounted on a pod.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims"&gt;PersistentVolumeClaim&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A PersistentVolumeClaim binds a PersistentVolume to a pod, which can then actively use it while running.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/"&gt;ConfigMap&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Environment Variables&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A ConfigMap defines widely accessible properties.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Secret&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Secured Env. Var.&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;A Secret defines widely accessible properties with potential encryption and access limitations.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/reference/glossary/?all=true"&gt;Official Kubernetes Glossary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/"&gt;Official Concepts Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Definition files&lt;/h2&gt;
&lt;p&gt;The resources in Kubernetes are created in a declarative fashion, and while it is possible to configure your application deployment through the command line,
a good practice is to keep track of the resource definitions in a versioned environment.
Sometimes named &lt;a href="https://www.gitops.tech/"&gt;GitOps&lt;/a&gt;, this practice is not only applicable for Kubernetes but widely applied for delivery systems,
backed up by the &lt;a href="https://aws.amazon.com/devops/what-is-devops/"&gt;DevOps&lt;/a&gt; movement.&lt;/p&gt;
&lt;p&gt;To this effect, Kubernetes proposes a &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html"&gt;YAML&lt;/a&gt; representation of the resource declaration, and its structure can be summarized as follow:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;File type&lt;/th&gt;
&lt;th&gt;Content&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;apiVersion&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;All files&lt;/td&gt;
&lt;td&gt;Version to use while parsing the file.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kind&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;All files&lt;/td&gt;
&lt;td&gt;Type of resource that the file is describing.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;All files&lt;/td&gt;
&lt;td&gt;Resource identification and labeling.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data centric files (Secret, ConfigMap)&lt;/td&gt;
&lt;td&gt;Content entry point for data mapping.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;spec&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Most files (Pod, Deployment, Ingress, ...)&lt;/td&gt;
&lt;td&gt;Content entry point for resource configuration.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Watch out: some resources such as StorageClass do no use a single entry point as described above&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html"&gt;Guide on apiVersion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://yaml.org/spec/1.2/spec.html"&gt;Yaml Specifications&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Metadata and labels&lt;/h3&gt;
&lt;p&gt;The metadata entry is critical while creating any resource as it will enable Kubernetes and yourself to easily identify and select the resource.&lt;/p&gt;
&lt;p&gt;In this entry, you will define a &lt;code&gt;name&lt;/code&gt; and a &lt;code&gt;namespace&lt;/code&gt; (defaults to &lt;code&gt;default&lt;/code&gt;),
thanks to which the control plane will automatically be able to tell if the file is a new addition to the cluster or the revision of a previously loaded file.&lt;/p&gt;
&lt;p&gt;On top of those elements, you can define a &lt;code&gt;labels&lt;/code&gt; section.
It is composed of a set of key-value pairs to narrow down the context and content of your resource.
Those labels can later be used in almost any CLI commands through &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors"&gt;Selectors&lt;/a&gt;.
As those entries are not used in the core behavior of Kubernetes,
you can use any name you want, even if Kubernetes defines some &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/"&gt;best practices recommendations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, you can also create an &lt;code&gt;annotations&lt;/code&gt; section, which is almost identical to &lt;code&gt;labels&lt;/code&gt; but not used by Kubernetes at all.
Those can be used on the applicative side to trigger behaviors or simply add data to ease debugging.&lt;/p&gt;
&lt;p&gt;```yaml&lt;/p&gt;
&lt;h1&gt;&lt;metadata&gt; narrows down selection and identify the resource&lt;/h1&gt;
&lt;p&gt;metadata:
  # The &lt;name&gt; entry is required and used to identify the resource
  name: my-resource
  namespace: my-namespace-or-default
  # &lt;labels&gt; is optional but often needed for resource selection
  labels:
    app: application-name
    category: back
  # &lt;annotations&gt; is optional and not needed for the configuration of Kubernetes
  annotations:
    version: 4.2
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/"&gt;Naming and Identification&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/"&gt;Labels and Selectors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/"&gt;Annotations&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Data centric configuration files&lt;/h3&gt;
&lt;p&gt;Those files define key-value mappings that can be used later in other resources.
Usually, those resources (&lt;em&gt;i.e.&lt;/em&gt; Secrets and ConfigMap) are loaded before anything else, 
as it is more likely than not that your infrastructure files are dependent on them.&lt;/p&gt;
&lt;p&gt;```yaml
apiVersion: v1&lt;/p&gt;
&lt;h1&gt;&lt;kind&gt; defines the resource described in this file&lt;/h1&gt;
&lt;p&gt;kind: ConfigMap
metadata:
  name: my-config
data:
  # &lt;data&gt; configures data to load
  configuration_key: "configuration_value"
  properties_entry: |
    # Any multiline content is accepted
    multiline_config=true
```&lt;/p&gt;
&lt;h3&gt;Infrastructure centric configuration files&lt;/h3&gt;
&lt;p&gt;Those files define the infrastructure to deploy on the cluster, potentially using content from the data files.&lt;/p&gt;
&lt;p&gt;```yaml
apiVersion: v1&lt;/p&gt;
&lt;h1&gt;&lt;kind&gt; defines the resource described in this file&lt;/h1&gt;
&lt;p&gt;kind: Pod
metadata:
  name: my-web-server
spec:
  # &lt;spec&gt; is a domain specific description of the resource.
  # The specification entries will be very different from one kind to another
```&lt;/p&gt;
&lt;h2&gt;Resources definition&lt;/h2&gt;
&lt;p&gt;In this section, we will take a closer look at the configuration of the most used resources on a Kubernetes application.
This is also the occasion to showcase the interactions between resources.&lt;/p&gt;
&lt;p&gt;At the end of the section, we will have a running Nginx server and will be able to contact the server from outside the cluster.
The following diagram summarizes the intended state:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Intended Deployment" src="/images/posts/2021-05-10_Kubernetes-Apprentice-Cookbook/kube_nginx.svg" /&gt;&lt;/p&gt;
&lt;h3&gt;ConfigMap&lt;/h3&gt;
&lt;p&gt;ConfigMap is used to hold properties that can be used later in your resources.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simple-web-config
  namespace: default
data:
  configuration_key: "Configuration value"&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The configuration defined above can then be &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmaps-and-pods"&gt;selected from another resource definition&lt;/a&gt; with the following snippet:
&lt;code&gt;yaml
valueFrom:
  configMapKeyRef:
    name: simple-web-config
    key: configuration_key&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: ConfigMaps are only available in the namespace in which they are defined.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/"&gt;ConfigMap Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Secret&lt;/h3&gt;
&lt;p&gt;All sensitive data should be put in Secret files (e.g. API keys, passphrases, …). 
By default, the data is simply held as base64 encoded values without encryption. 
However, Kubernetes proposes ways of mitigating leakage risks by
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/"&gt;integrating a Role-Based Access Control&lt;/a&gt;
or &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/"&gt;encrypting secrets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Secret file defines a &lt;code&gt;type&lt;/code&gt; key at its root, which can be used to add validation on the keys declared in the &lt;code&gt;data&lt;/code&gt; entry.
By default, the type is set to &lt;code&gt;Opaque&lt;/code&gt; which does not validate the entries at all.&lt;/p&gt;
&lt;p&gt;```yaml
apiVersion: v1
kind: Secret
metadata:
  name: simple-web-secrets&lt;/p&gt;
&lt;h1&gt;Opaque &lt;type&gt; can hold generic secrets, so no validation will be done.&lt;/h1&gt;
&lt;p&gt;type: Opaque
data:
  # Secrets should be encoded in base64
  secret_configuration_key: "c2VjcmV0IHZhbHVl"
```&lt;/p&gt;
&lt;p&gt;The secret defined above can then be &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables"&gt;selected from another resource definition&lt;/a&gt; with the following snippet:
&lt;code&gt;yaml
valueFrom:
  secretKeyRef:
    name: simple-web-secrets
    key: secret_configuration_key&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Secrets are only available in the namespace in which they are defined.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Secrets Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-types"&gt;Available Secret Types&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Pod&lt;/h3&gt;
&lt;p&gt;A Pod definition file is pretty straightforward but can become pretty big due to the quantity of configuration available.
The &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;image&lt;/code&gt; fields are the only mandatory ones, but you might commonly use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ports&lt;/code&gt; to define the ports to open on both the container and pod. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;env&lt;/code&gt; to define the environment variables to load on the container.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;args&lt;/code&gt; and &lt;code&gt;entrypoint&lt;/code&gt; to customize the container startup sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pods are usually not created as standalone resources on Kubernetes,
as the best practice indicates to &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/#working-with-pods"&gt;use pod as part of higher level definition&lt;/a&gt;
(&lt;em&gt;e.g.&lt;/em&gt; Deployment).
In those cases, the Pod file's content will simply be embedded in the other resource's file.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-web-server
spec:
  # &amp;lt;containers&amp;gt; is a list of container definition to embed in the pod
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
      env:
        - name: SOME_CONFIG
          # Create a line "value: &amp;lt;config_entry&amp;gt;" from the ConfigMap data
          valueFrom:
            configMapKeyRef:
              name: simple-web-config
              key: configuration_key
        - name: SOME_SECRET
          # Create a line "value: &amp;lt;config_entry&amp;gt;" from the Secret data
          valueFrom:
            secretKeyRef:
              name: simple-web-secrets
              key: secret_configuration_key&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Pods are only available in the namespace in which they are defined.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;Pod Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/"&gt;Advanced Pod Configuration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#podspec-v1-core"&gt;Fields available in Pod &amp;lt;spec&amp;gt; entry&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#container-v1-core"&gt;Fields available in Pod &amp;lt;containers&amp;gt; entry&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Deployment&lt;/h3&gt;
&lt;p&gt;The Deployment is generally used as the atomic working unit since it will automatically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a pod definition based on the &lt;code&gt;template&lt;/code&gt; entry.&lt;/li&gt;
&lt;li&gt;Create a ReplicaSet on pods selected by the &lt;code&gt;selector&lt;/code&gt; entry, with the value of &lt;code&gt;replicas&lt;/code&gt; as a count of pods that should be running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following file requests 3 instances of an Nginx server running at all times.
The file may look a bit heavy, but most of it is the Pod definition copied from above.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-web-server-deployment
  namespace: default
  labels:
    app: webserver
spec:
  # &amp;lt;selector&amp;gt; should retrieve the Pod defined below, and possibly more
  selector:
    matchLabels:
      app: webserver
      instance: nginx-ws-deployment
  # &amp;lt;replicas&amp;gt; asks for 3 pods running in parallel at all time
  replicas: 3
  # The content of &amp;lt;template&amp;gt; is a Pod definition file, without &amp;lt;apiVersion&amp;gt; nor &amp;lt;kind&amp;gt;
  template:
    metadata:
      name: my-web-server
      namespace: default
      labels:
        app: webserver
        instance: nginx-ws-deployment
    spec:
      containers:
        - name: web
          image: nginx
          ports:
            - name: web
              containerPort: 80
              protocol: TCP
          env:
            - name: SOME_CONFIG
              # Create a line "value: &amp;lt;config_entry&amp;gt;" from the ConfigMap data
              valueFrom:
                configMapKeyRef:
                  name: simple-web-config
                  key: configuration_key
            - name: SOME_SECRET
              # Create a line "value: &amp;lt;config_entry&amp;gt;" from the Secret data
              valueFrom:
                secretKeyRef:
                  name: simple-web-secrets
                  key: secret_configuration_key&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Deployments are only available in the namespace in which they are defined.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Service&lt;/h3&gt;
&lt;p&gt;A pod might be deleted and recreated at any time.
When it occurs the pod's IP address will change, which could result in a loss of connection if you are directly contacting it.
To solve this issue, a Service provides a stable contact point to a set of Pods, while remaining agnostic of their state and configuration.
Usually, Pods are chosen to be part of a Service through a &lt;code&gt;selector&lt;/code&gt; entry, thus based on its &lt;code&gt;labels&lt;/code&gt;.
A Pod is selected if and only if all the labels in the selector are worn by the pod.&lt;/p&gt;
&lt;p&gt;There are three types of services that are acting quite differently, among which you can select using the type entry.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;ClusterIP&lt;/strong&gt; service is bound to an internal IP from the cluster, hence only internally reachable.
This is the type of service created by default and is suitable for binding different applications inside the same cluster.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;NodePort&lt;/strong&gt; service will bind a port (by default in range 30000 to 32767) on the nodes hosting the selected pods.
This enables you to contact the service directly through the node IP.
That also means that your service will be as accessible as the virtual or physical machines hosting those pods.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Using NodePort can pose security risks, as it enables a direct connection from outside the cluster.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;LoadBalancer&lt;/strong&gt; service will automatically create a &lt;a href="https://www.nginx.com/resources/glossary/load-balancing/"&gt;load balancer&lt;/a&gt;
instance from the cloud service provider on which the cluster is running.
This load balancer is created outside the cluster but will automatically be bound to the nodes hosting the selected pods.&lt;/p&gt;
&lt;p&gt;This is an easy way to expose your service but can end up being costly as each service will be managed by a single load balancer.&lt;/p&gt;
&lt;p&gt;If you are setting up your own Ingress as we will do here, you may want to use a &lt;code&gt;ClusterIp&lt;/code&gt; service, as other services are made for specific use cases.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yaml
apiVersion: v1
kind: Service
metadata:
  name: simple-web-service-clusterip
spec:
  # ClusterIP is the default service &amp;lt;type&amp;gt;
  type: ClusterIP
  # Select all pods declaring a &amp;lt;label&amp;gt; entry "app: webserver"
  selector:
    app: webserver
  ports:
    - name: http
      protocol: TCP
      # &amp;lt;port&amp;gt; is the port to bind on the service side
      port: 80
      # &amp;lt;targetPort&amp;gt; is the port to bind on the Pod side
      targetPort: 80&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Services are defined in a namespace but can be contacted from other namespaces.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;Service Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0"&gt;In Depth Service Comparison&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/"&gt;Create an External Load Balancer&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Ingress&lt;/h3&gt;
&lt;p&gt;Ingress enables you to publish internal services without necessarily using a load balancer from cloud service providers.
You usually need only one ingress per namespace, where you can bind as many routing &lt;code&gt;rules&lt;/code&gt; and &lt;code&gt;backends&lt;/code&gt; as you want.
A backend will typically be an internally routed &lt;code&gt;ClusterIP&lt;/code&gt; service.&lt;/p&gt;
&lt;p&gt;Please note that Kubernetes does not handle ingress resources by itself and relies on third-party implementations.
As a result, you will have to choose and install an &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"&gt;Ingress Controller&lt;/a&gt;
before using any ingress resource.
On the other hand, it makes the ingress resource customizable depending on the needs of your cluster.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: simple-web-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    # Using &amp;lt;host&amp;gt; redirects all request matching the given DNS name to this rule
    - host: "*.minikube.internal"
      http:
        paths:
          - path: /welcome
            pathType: Prefix
            backend:
              service:
                name: simple-web-service-clusterip
                port:
                  number: 80
    # All other requests will be redirected through this rule
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: simple-web-service-clusterip
                port:
                  number: 80&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Ingresses are defined in the namespace but may contact services from other namespaces and are publicly accessible outside the cluster.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"&gt;Available Ingress Controllers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/"&gt;Enable Ingress on Minikube&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.github.io/ingress-nginx/examples/rewrite/"&gt;Nginx Ingress Annotations&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;CLI Usage&lt;/h2&gt;
&lt;h3&gt;Create and manage resources&lt;/h3&gt;
&lt;p&gt;This section showcases the basic CLI commands to manipulate resources. 
As said before, while it is possible to manually manage resources, a better practice is to use files.&lt;/p&gt;
&lt;p&gt;```shell&lt;/p&gt;
&lt;h1&gt;&lt;kind&gt; is the type of resource to create (e.g. deployment, secret, namespace, quota, ...)&lt;/h1&gt;
&lt;p&gt;$ kubectl create &lt;kind&gt; &lt;name&gt;
$ kubectl edit   &lt;kind&gt; &lt;name&gt;
$ kubectl delete &lt;kind&gt; &lt;name&gt;&lt;/p&gt;
&lt;h1&gt;All those commands can be used through a description file.&lt;/h1&gt;
&lt;p&gt;$ kubectl create -f &lt;resource&gt;.yaml
$ kubectl edit   -f &lt;resource&gt;.yaml
$ kubectl delete -f &lt;resource&gt;.yaml
```&lt;/p&gt;
&lt;p&gt;To ease resources manipulations through files, you can reduce the interactions to the CLI to the two following commands:&lt;/p&gt;
&lt;p&gt;```shell&lt;/p&gt;
&lt;h1&gt;Create and update any resource&lt;/h1&gt;
&lt;p&gt;$ kubectl apply   -f &lt;resource&gt;.yaml&lt;/p&gt;
&lt;h1&gt;Delete any resource&lt;/h1&gt;
&lt;p&gt;$ kubectl delete  -f &lt;resource&gt;.yaml
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/"&gt;Managing Resources&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Monitor and Debug&lt;/h3&gt;
&lt;h4&gt;Fetch resources&lt;/h4&gt;
&lt;p&gt;You can see all resources running through the CLI using &lt;code&gt;kubectl get &amp;lt;kind&amp;gt;&lt;/code&gt;.
This command is pretty powerful and lets you filter the kind of resources to display or select the resources you want to see.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: if not specified, Kubernetes will work on the &lt;code&gt;default&lt;/code&gt; namespace. You can specify &lt;code&gt;-n &amp;lt;namespace&amp;gt;&lt;/code&gt; to work on a specific namespace or &lt;code&gt;-A&lt;/code&gt; to show every namespace.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;```shell&lt;/p&gt;
&lt;h1&gt;Fetch everything&lt;/h1&gt;
&lt;p&gt;$ kubectl get all
NAME                                            READY   STATUS    RESTARTS   AGE
pod/my-web-server-deployment-58c4fd887f-5vm2b   1/1     Running   0          128m
pod/my-web-server-deployment-58c4fd887f-gq6lr   1/1     Running   0          128m
pod/my-web-server-deployment-58c4fd887f-gs6qb   1/1     Running   0          128m&lt;/p&gt;
&lt;p&gt;NAME                                   TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/simple-web-service-clusterip   ClusterIP      10.96.96.241     &lt;none&gt;        80/TCP,443/TCP               60m
service/simple-web-service-lb          LoadBalancer   10.108.182.232   &lt;pending&gt;     80:31095/TCP,443:31940/TCP   60m
service/simple-web-service-np          NodePort       10.101.77.203    &lt;none&gt;        80:31899/TCP,443:31522/TCP   60m&lt;/p&gt;
&lt;p&gt;NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-web-server-deployment   3/3     3            3           136m&lt;/p&gt;
&lt;p&gt;NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/my-web-server-deployment-58c4fd887f   3         3         3       128m&lt;/p&gt;
&lt;h1&gt;We can ask for more details&lt;/h1&gt;
&lt;p&gt;$ kubectl get deployment -o wide
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES  SELECTOR
my-web-server-deployment   3/3     3            3           121m   web          nginx   app=webserver&lt;/p&gt;
&lt;h1&gt;Some resources are not visible using "all" but available&lt;/h1&gt;
&lt;p&gt;$ kubectl get configmap
NAME                DATA   AGE
kube-root-ca.crt    1      38d
simple-web-config   3      3h17m
```&lt;/p&gt;
&lt;h4&gt;Dig into a particular resource&lt;/h4&gt;
&lt;p&gt;This section will show you how to dig into resources.
Most of the required day-to-day operations are doable through the three following commands.&lt;/p&gt;
&lt;p&gt;The first command will give you the resource's complete configuration, using &lt;code&gt;kubectl describe &amp;lt;kind&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;```shell&lt;/p&gt;
&lt;h1&gt;Let's describe the ingress for the sake of example&lt;/h1&gt;
&lt;p&gt;$ kubectl describe ingress/simple-web-ingress
Name:             simple-web-ingress
Namespace:        default
Address:          192.168.64.2
Default backend:  default-http-backend:80 (&lt;error: endpoints "default-http-backend" not found&gt;)
Rules:
  Host                 Path  Backends
  ----                 ----  --------
  *.minikube.internal
                       /welcome   simple-web-service-clusterip:80 (172.17.0.4:80,172.17.0.5:80,172.17.0.6:80 + 1 more...)
  *
                       /   simple-web-service-clusterip:80 (172.17.0.4:80,172.17.0.5:80,172.17.0.6:80 + 1 more...)
Annotations:           nginx.ingress.kubernetes.io/rewrite-target: /
Events:
  Type    Reason  Age                 From                      Message
  ----    ------  ----                ----                      -------
  Normal  UPDATE  7m6s (x6 over 23h)  nginx-ingress-controller  Ingress default/simple-web-ingress
```&lt;/p&gt;
&lt;p&gt;Another important command is &lt;code&gt;kubectl logs &amp;lt;kind&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;, as you might expect it shows you the resources' logs if applicable.
As the logs are produced by Pods,
running such a command on a resource above a Pod will dig through Kubernetes to display the logs of a randomly chosen Pod underneath it.&lt;/p&gt;
&lt;p&gt;```shell
$ kubectl logs deployments/my-web-server-deployment
Found 3 pods, using pod/my-web-server-deployment-755b499f77-4n5vn&lt;/p&gt;
&lt;h1&gt;[logs]&lt;/h1&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;Finally, it is sometimes useful to connect on a pod, you can do so with the command kubectl &lt;code&gt;exec -it &amp;lt;pod_name&amp;gt; -- /bin/bash&lt;/code&gt;.
This will open an interactive shell on the pod, enabling you to interact with its content.&lt;/p&gt;
&lt;p&gt;```shell&lt;/p&gt;
&lt;h1&gt;As for logs, when called on any resource enclosing Pods,&lt;/h1&gt;
&lt;h1&gt;Kubernetes will randomly chose one to  execute the action&lt;/h1&gt;
&lt;p&gt;$ kubectl exec -it deployment/my-web-server-deployment -- /bin/bash
root@my-web-server-deployment-56c4554cf9-qwtm6:/# ls&lt;/p&gt;
&lt;h1&gt;[...]&lt;/h1&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;During this article, we saw the fundamentals behind deploying and publishing stateless services using Kubernetes.
But you can do a lot more complex things with Kubernetes.
If you want to learn more about it, I can recommend you to look at these resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the &lt;a href="https://kubernetes.io/docs/reference/"&gt;Kubernetes reference documentation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Install a sandbox locally with &lt;a href="https://kubernetes.io/fr/docs/setup/learning-environment/minikube/"&gt;Minikube&lt;/a&gt;, and play with it.&lt;/li&gt;
&lt;li&gt;Watch the video &lt;a href="https://youtu.be/X48VuDVv0do"&gt;Kubernetes Tutorial for Beginners - &lt;em&gt;TechWorld with Nana&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Manually bootstrap a Kubernetes cluster: &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way"&gt;Kubernetes The Hard Way&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incidentally, there are multiple subjects I could not deeply talk about in this article and that may be of interest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On the developer side:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/"&gt;Volumes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"&gt;StatefulSets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/"&gt;Selectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;On the cluster administrator side:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/"&gt;Access control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/"&gt;Secret encryption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/"&gt;Quotas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/"&gt;Network Plugins&lt;/a&gt;
  (&lt;em&gt;e.g.&lt;/em&gt; &lt;a href="https://github.com/contiv/vpp/blob/master/docs/ARCHITECTURE.md"&gt;VPP&lt;/a&gt; and &lt;a href="https://github.com/weaveworks/weave"&gt;Weaveworks&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, if you are interested in the ecosystem around Kubernetes,
you may want to take a look at the following technologies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.openshift.com/blog/enterprise-kubernetes-with-openshift-part-one"&gt;Openshift&lt;/a&gt;
  is wrapping Kubernetes with production friendly features. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://helm.sh/"&gt;Helm&lt;/a&gt;
  is a charts manager for Kubernetes helping improve re-usability of configuration files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://argoproj.github.io/argo-cd/"&gt;ArgoCD&lt;/a&gt;
  is keeping your Kubernetes Cluster up to date with your configurations from Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Resources' repository&lt;/h3&gt;
&lt;p&gt;The resources definitions used in this article are available in the following &lt;a href="https://github.com/aveuiller/frameworks-bootstrap/tree/feat_integrate_kube/Kubernetes"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;CLI equivalents - Docker and Kubernetes&lt;/h3&gt;
&lt;p&gt;Managing containers with Docker and pods with Kubernetes is very similar,
as you can see on the following table describing equivalent operations between both technologies.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;Docker&lt;/th&gt;
&lt;th&gt;Kubernetes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Running containers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker ps&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kubectl get pods&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Configuration details&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker inspect &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kubectl describe &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Show logs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker logs &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kubectl logs &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Enter container&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker exec -it &amp;lt;name&amp;gt; /bin/bash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kubectl exec -it &amp;lt;name&amp;gt; -- /bin/bash&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Thanks to Sarra Habchi, Dimitri Delabroye, and Alexis Geffroy for the reviews&lt;/em&gt;&lt;/p&gt;</content><category term="Software Engineering"></category><category term="Cheat sheet"></category><category term="DevOps"></category><category term="Kubernetes"></category><category term="Cloud"></category></entry><entry><title>Spring Boot: Apprentice Cookbook</title><link href="https://aveuiller.github.io/spring_boot_apprentice_cookbook.html" rel="alternate"></link><published>2021-01-27T00:00:00+01:00</published><updated>2021-01-27T00:00:00+01:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2021-01-27:/spring_boot_apprentice_cookbook.html</id><summary type="html">&lt;p&gt;Spring Boot cheat sheet to bootstrap an API&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Medium: &lt;a href="https://medium.com/@aveuiller/spring-boot-apprentice-cookbook-61db5a3f6450"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img alt="Spring Boot logo" src="https://cdn-images-1.medium.com/max/800/1*gxXLMIuJDHCH7fwIgEP1cg.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spring.io/projects/spring-boot"&gt;Spring Boot&lt;/a&gt; is a web framework built on top of the framework &lt;a href="https://spring.io/projects/spring-framework"&gt;Spring&lt;/a&gt;. It is designed for easier use and quicker implementation. It does so by configuring the application and its environment as automatically as possible. As a newcomer, I can say that it makes the framework really easy to get into.&lt;/p&gt;
&lt;p&gt;My learning led me to read most of the &lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#using-boot-structuring-your-code"&gt;reference documentation&lt;/a&gt;, which is well written and gives you a lot of insights into the internal behavior of Spring Boot. This documentation gives a lot of details, so this article aims to take the counter approach and pinpoint the concepts you will need to implement an API using Spring Boot. I will complement each section with a set of links to related documentation, may you want to dig further.&lt;/p&gt;
&lt;p&gt;As a side note, this document will be using version 2.4.2 of the framework, on a Java project using Gradle as the build system. 
However, the information remains applicable to any compatible language and build system.&lt;/p&gt;
&lt;p&gt;This article will cover the following aspects of creating an API with Spring Boot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bootstrap the project&lt;/li&gt;
&lt;li&gt;Create REST endpoints&lt;/li&gt;
&lt;li&gt;Handle errors&lt;/li&gt;
&lt;li&gt;Connect to a persistence layer&lt;/li&gt;
&lt;li&gt;Paginate the results&lt;/li&gt;
&lt;li&gt;Test the application&lt;/li&gt;
&lt;li&gt;Package the Application&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Bootstrap the project&lt;/h3&gt;
&lt;p&gt;This part may be the easiest, as Spring Boot is providing a package generator at &lt;a href="https://start.spring.io/"&gt;https://start.spring.io/&lt;/a&gt;. We can select all required modules and retrieve an archived project with the build system, dependencies, and main application class.&lt;/p&gt;
&lt;p&gt;Outside of this generator, to declare a RESTful API, our project should define the Spring Boot &lt;em&gt;starter web&lt;/em&gt; dependency. 
The &lt;em&gt;starter&lt;/em&gt; dependencies are a set of ready to use features packaged by Spring Boot.&lt;/p&gt;
&lt;p&gt;```groovy
plugins {
  id 'org.springframework.boot' version '2.4.2'
}&lt;/p&gt;
&lt;p&gt;dependencies {
  implementation 'org.springframework.boot:spring-boot-starter-web'
}
```&lt;/p&gt;
&lt;p&gt;The application’s main method should be contained in any class, on which we should apply the annotation &lt;code&gt;@SpringBootApplication&lt;/code&gt;. This annotation is responsible for a lot of automatic configurations, namely the components injection and web server startup.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
@SpringBootApplication
public class MyApplication {
    public static void main(String[] args) {
        SpringApplication.run(MyApplication.class, args);
    }
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Starting the server is as simple as using the embedded command &lt;code&gt;./gradlew bootRun&lt;/code&gt;. 
The server will start, but we don’t have any endpoint to serve at the moment.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#using-boot-using-springbootapplication-annotation"&gt;@SpringBootApplication&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#using-boot-starter"&gt;List of starter dependencies&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Create a REST endpoint&lt;/h3&gt;
&lt;p&gt;To create a controller, we simply have to annotate any class with &lt;code&gt;@RestController&lt;/code&gt;.
We can then configure any method inside this controller as an endpoint using &lt;code&gt;@RequestMapping&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;@RequestMapping&lt;/code&gt; help us configuring the endpoint by providing an URL, the HTTP verb, the expected data type, and more. 
It can be applied both on a class and a method, the configurations applied on the class will be inherited by the methods underneath and the path concatenated.&lt;/p&gt;
&lt;p&gt;To control our endpoint status codes we will return a&lt;code&gt;ResponseEntity&lt;/code&gt;, holding both the response message and &lt;code&gt;HttpStatus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;```java
@RestController
@RequestMapping(value = "/hello",
        consumes = MediaType.ALL_VALUE,
        produces = MediaType.APPLICATION_JSON_VALUE)
public class HelloWorldController {&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "/world", method = RequestMethod.GET)
    public ResponseEntity&lt;Map\&lt;String, String&gt;&gt; index() {
        HashMap&lt;String, String&gt; output = new HashMap&amp;lt;&amp;gt;();
        output.put("message", "Hello World!");
        return new ResponseEntity&amp;lt;&amp;gt;(output, HttpStatus.OK);
    }
}
```&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ResponseEntity&lt;/code&gt; will be automatically transformed to an HTTP response, using the &lt;code&gt;HttpStatus&lt;/code&gt; as response code and transforming the message to a JSON object.
On top of transforming &lt;em&gt;Maps&lt;/em&gt; to JSON objects, Spring Boot configure &lt;a href="https://github.com/FasterXML/jackson"&gt;Jackson&lt;/a&gt; to map all &lt;code&gt;public&lt;/code&gt; attributes or getters of any class to a JSON object.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;shell
$ curl -i "localhost:8080/hello/world"
HTTP/1.1 200
{"Hello":"World"}&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#getting-started-first-application-annotations"&gt;@RestController and @RequestMapping&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/bind/annotation/RequestMapping.html"&gt;@RequestMapping API doc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#howto-customize-the-jackson-objectmapper"&gt;Customize Json Serialization&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Advanced endpoint configuration&lt;/h3&gt;
&lt;p&gt;Now that we have a controller, we may want to define dynamic HTTP endpoints. To do so, the main annotations to keep in mind are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;@RequestBody&lt;/code&gt; : Defines a body structure through a java Class.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;@PathVariable&lt;/code&gt;: Defines a variable subpart of the endpoint URL.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;@RequestParam&lt;/code&gt; : Defines a query parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The controller below showcases the three annotations with two endpoints, each returning a custom “Hello World” depending on the query.&lt;/p&gt;
&lt;p&gt;````java
@RestController
@RequestMapping(value = "/hello",
        consumes = MediaType.ALL_VALUE,
        produces = MediaType.APPLICATION_JSON_VALUE)
public class HelloWorldController {&lt;/p&gt;
&lt;p&gt;// The behavior is not representative of a typical POST request
    // and only here as a matter of example.
    @RequestMapping(value = "", method = RequestMethod.POST)
    public ResponseEntity&lt;Map\&lt;String, String&gt;&gt; greetFromBody(@RequestBody HelloBody helloBody) {
        HashMap&lt;String, String&gt; output = new HashMap&amp;lt;&amp;gt;();
        output.put("message", "Hello " + helloBody.getName());
        return new ResponseEntity&amp;lt;&amp;gt;(output, HttpStatus.OK);
    }&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "/{name}", method = RequestMethod.GET)
    public ResponseEntity&lt;Map\&lt;String, String&gt;&gt; greet(@PathVariable String name,
                                                     @RequestParam(required = false,
                                                                   defaultValue = "0") int amount_exclamation) {
        HashMap&lt;String, String&gt; output = new HashMap&amp;lt;&amp;gt;();
        StringBuilder b = new StringBuilder("Hello ");
        b.append(name);
        for (int i = 0; i &amp;lt; amount_exclamation; i++) {
            b.append("!");
        }
        output.put("message", b.toString());
        return new ResponseEntity&amp;lt;&amp;gt;(output, HttpStatus.OK);
    }
}&lt;/p&gt;
&lt;p&gt;class HelloBody {
    String name;&lt;/p&gt;
&lt;p&gt;public HelloBody() {
        // Used by Jackson
    }&lt;/p&gt;
&lt;p&gt;public String getName() {
        return this.name;
    }
}
````&lt;/p&gt;
&lt;p&gt;The endpoints defined above can be used as follows:&lt;/p&gt;
&lt;p _message_:_Hello="&amp;quot;message&amp;quot;:&amp;quot;Hello" jack_="jack!!!!&amp;quot;"&gt;```shell
curl -i "localhost:8080/hello/jack?amount_exclamation=4"
HTTP/1.1 200&lt;/p&gt;
&lt;h1&gt;-d automatically creates a POST request.&lt;/h1&gt;
&lt;p&gt;$ curl -i "localhost:8080/hello" -d '{"name": "Bob"}' -H "Content-Type: application/json"
HTTP/1.1 200
{"message":"Hello Bob"}
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/5.2.8.RELEASE/spring-framework-reference/web.html#mvc-ann-requestbody"&gt;@RequestBody&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/5.2.8.RELEASE/spring-framework-reference/web.html#mvc-ann-requestmapping-uri-templates"&gt;@PathVariable&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/5.2.8.RELEASE/spring-framework-reference/web.html#mvc-ann-requestparam"&gt;@RequestParam&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Handle errors&lt;/h3&gt;
&lt;p&gt;By default, Spring Boot will return the HTTP code 200 for any successful request, 404 if the endpoint is not registered, and 500 for any error. We already saw that using &lt;code&gt;ResponseEntity&lt;/code&gt; enables us to override this behavior for successful requests, but we still need to handle error codes more finely.&lt;/p&gt;
&lt;p&gt;To do so, we will define custom API exceptions that will be automatically transformed into HTTP codes. 
This transformation is done by a class extending &lt;code&gt;ResponseEntityExceptionHandler&lt;/code&gt; and annotated with &lt;code&gt;@ControllerAdvice&lt;/code&gt;. 
In this class, we can define methods to handle exceptions using the annotations &lt;code&gt;@ExceptionHandler&lt;/code&gt; and &lt;code&gt;@ResponseStatus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;```java
@ControllerAdvice
public class MyApplicationControllerAdvice extends ResponseEntityExceptionHandler {&lt;/p&gt;
&lt;p&gt;@ExceptionHandler(ApiException.class)
    @ResponseStatus(HttpStatus.BAD_REQUEST)
    public void handleBadRequest() {
    }&lt;/p&gt;
&lt;p&gt;@ExceptionHandler(NotFoundException.class)
    @ResponseStatus(HttpStatus.NOT_FOUND)
    public void handleNotFound() {
    }
}&lt;/p&gt;
&lt;p&gt;public class ApiException extends Exception {
}&lt;/p&gt;
&lt;p&gt;public class NotFoundException extends ApiException {
}
```&lt;/p&gt;
&lt;p&gt;After defining the &lt;code&gt;ControllerAdvice&lt;/code&gt; in your project, any exception thrown by your controllers will be parsed and transformed to the bound &lt;code&gt;ResponseStatus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;```java
@RestController
@RequestMapping(value = "/exception")
public class ExceptionController {&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "/404", method = RequestMethod.GET)
    public ResponseEntity&lt;Map\&lt;String, String&gt;&gt; notFound() throws NotFoundException {
        throw new NotFoundException();
    }    &lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "/400", method = RequestMethod.GET)
    public ResponseEntity&lt;Map\&lt;String, String&gt;&gt; badRequest() throws ApiException {
        throw new ApiException();
    }&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "/500", method = RequestMethod.GET)
    public ResponseEntity&lt;Map\&lt;String, String&gt;&gt; ise() throws Exception {
        throw new Exception();
    }
}
```&lt;/p&gt;
&lt;p&gt;```shell
$ curl -i "localhost:8080/exception/500"
HTTP/1.1 500&lt;/p&gt;
&lt;p&gt;$ curl -i "localhost:8080/exception/404"
HTTP/1.1 404&lt;/p&gt;
&lt;p&gt;$ curl -i "localhost:8080/exception/400"
HTTP/1.1 400
```&lt;/p&gt;
&lt;p&gt;Our exception handling is very simple and does not return any payload, but it is possible to implement exception parsing in the methods of &lt;code&gt;ResponseEntityExceptionHandler&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-error-handling"&gt;ResponseEntityExceptionHandler&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/5.2.8.RELEASE/spring-framework-reference/web.html#mvc-ann-controller-advice"&gt;@ControllerAdvice&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/5.2.8.RELEASE/spring-framework-reference/web.html#mvc-ann-exceptionhandler"&gt;@ExceptionHandler&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/5.2.8.RELEASE/spring-framework-reference/web.html#mvc-ann-exceptionhandler-return-values"&gt;@ResponseStatus&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Connect to a persistence layer&lt;/h3&gt;
&lt;h4&gt;Configuration&lt;/h4&gt;
&lt;p&gt;To use a database, we will need the &lt;em&gt;Java Persistence API&lt;/em&gt; (JPA) package and the implementation of any persistence layer.
The former will install interface APIs, while the latter will provide the implementations and drivers.&lt;/p&gt;
&lt;p&gt;To pinpoint the minimal changes required to switch between two distinct databases, we will show the integration with both &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; and &lt;a href="https://www.h2database.com/html/main.html"&gt;H2&lt;/a&gt; at the same time.
First, let’s declare our dependencies:&lt;/p&gt;
&lt;p&gt;```groovy
dependencies {
  implementation 'org.springframework.boot:spring-boot-starter-data-jpa'&lt;/p&gt;
&lt;p&gt;// Dependencies to your used dbms
  implementation 'org.postgresql:postgresql:42.2.1'
  implementation 'com.h2database:h2:1.4.200'
}
```&lt;/p&gt;
&lt;p&gt;The second step is to configure the accesses in &lt;code&gt;application.properties&lt;/code&gt;. The property file is the first and the last time we will have to worry about our persistence configuration. In this file, the 3 lines commented out are the only part to change to switch from PostgreSQL to H2.&lt;/p&gt;
&lt;p&gt;```properties
spring.datasource.username=user
spring.datasource.password=password
spring.datasource.generate-unique-name=true&lt;/p&gt;
&lt;h1&gt;Automatically create &amp;amp; update the database schema from code.&lt;/h1&gt;
&lt;p&gt;spring.jpa.hibernate.ddl-auto=update&lt;/p&gt;
&lt;h1&gt;spring.datasource.url=jdbc:h2:mem:database_name&lt;/h1&gt;
&lt;p&gt;spring.datasource.url=jdbc:postgresql://localhost:5432/database_name&lt;/p&gt;
&lt;h1&gt;spring.datasource.driver-class-name=org.h2.Driver&lt;/h1&gt;
&lt;p&gt;spring.datasource.driver-class-name=org.postgresql.Driver&lt;/p&gt;
&lt;h1&gt;spring.jpa.database-platform=org.hibernate.dialect.H2Dialect&lt;/h1&gt;
&lt;p&gt;spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL10Dialect
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-configure-datasource"&gt;Database configuration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#data-properties"&gt;Available properties&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Define a model&lt;/h4&gt;
&lt;p&gt;Defining a model is as simple as using annotations defined on &lt;a href="https://www.jcp.org/en/jsr/detail?id=317"&gt;JSR-317&lt;/a&gt;. These annotations are available through the package &lt;em&gt;javax.persistence,&lt;/em&gt; which is available through the JPA dependency.&lt;/p&gt;
&lt;p&gt;For instance, the code below creates a &lt;em&gt;Delivery&lt;/em&gt; entity. Our entity identifier is the field &lt;em&gt;id&lt;/em&gt;, which will be automatically initialized and increased on each new saved entity in the database thanks to the annotation &lt;code&gt;@GeneratedValue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: All attributes publicly available will be set into the JSON representation of the entity in the API responses.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;```java
@Entity
@Table(name = "delivery")
public class Delivery {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    long id;&lt;/p&gt;
&lt;p&gt;@Column(nullable = false)
    @NotNull
    @Enumerated(EnumType.STRING)
    DeliveryState state;&lt;/p&gt;
&lt;p&gt;@Column(nullable = false)
    @NotNull
    String location;&lt;/p&gt;
&lt;p&gt;public Delivery() {
        // Used by Jackson2
    }&lt;/p&gt;
&lt;p&gt;public Delivery(@NotNull DeliveryState state, @NotNull String location) {
        this.state = state;
        this.location = location;
    }&lt;/p&gt;
&lt;p&gt;public long getId() {
        return id;
    }&lt;/p&gt;
&lt;p&gt;public DeliveryState getState() {
        return state;
    }&lt;/p&gt;
&lt;p&gt;public void setState(DeliveryState state) {
        this.state = state;
    }&lt;/p&gt;
&lt;p&gt;public String getLocation() {
        return location;
    }&lt;/p&gt;
&lt;p&gt;public void setLocation(String location) {
        this.location = location;
    }
}&lt;/p&gt;
&lt;p&gt;enum DeliveryState {
    PENDING, DELIVERING, WAITING_AT_ARRIVAL, RETURNING, RETURNED, PICKED_UP;
}
```&lt;/p&gt;
&lt;p&gt;To ensure consistency of our data class, we applied &lt;code&gt;@NotNull&lt;/code&gt; validations from &lt;a href="https://jcp.org/en/jsr/detail?id=303"&gt;JSR-303&lt;/a&gt;, these validations can be enforced on endpoints as we will see during the next section. 
The constraints are contained in the package &lt;em&gt;javax.validation.constraints&lt;/em&gt;, available through the dependency &lt;code&gt;spring-boot-starter-validation&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;groovy
dependencies {
  implementation 'org.springframework.boot:spring-boot-starter-validation'
}&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-entity-classes"&gt;Entity Declaration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://javaee.github.io/javaee-spec/javadocs/javax/persistence/package-summary.html"&gt;javax.persistence API documentation (@Entity, @Column, @Enumerate, …)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://javaee.github.io/javaee-spec/javadocs/javax/persistence/GeneratedValue.html"&gt;@GeneratedValue&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://javaee.github.io/javaee-spec/javadocs/javax/validation/constraints/package-summary.html"&gt;javax.validation.constraints API documentation (@NotNull)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Expose the model&lt;/h4&gt;
&lt;p&gt;To interact with our models, we have to define a &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.4.2/api/org/springframework/data/repository/Repository.html"&gt;Repository&lt;/a&gt;, for instance, a &lt;code&gt;CrudRepository&lt;/code&gt;.
Doing so is as easy as extending the class with an empty class. Spring Boot will automatically implement functions to interact with the entity.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
@Repository
public interface DeliveryRepository extends CrudRepository&amp;lt;Delivery, Long&amp;gt; {
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We annotate this component &lt;code&gt;@Repository&lt;/code&gt; to make it available to dependency injection. 
Then we can inject and use the repository in any class, for example directly in a controller. 
Using&lt;code&gt;@Autowired&lt;/code&gt; will automatically retrieve the &lt;code&gt;@Repository&lt;/code&gt; declared above_._&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; &lt;code&gt;_@Repository_&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code&gt;_@Service_&lt;/code&gt; &lt;em&gt;behave exactly as the main injection annotation&lt;/em&gt;&lt;code&gt;_@Component_&lt;/code&gt;&lt;em&gt;, it simply enables to mark a semantic difference.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;```java
@RestController
@RequestMapping(value = "/delivery",
        consumes = MediaType.APPLICATION_JSON_VALUE,
        produces = MediaType.APPLICATION_JSON_VALUE)
public class DeliveryController {&lt;/p&gt;
&lt;p&gt;private final DeliveryRepository deliveryRepository;&lt;/p&gt;
&lt;p&gt;@Autowired
    public DeliveryController(DeliveryRepository deliveryRepository) {
        this.deliveryRepository = deliveryRepository;
    }&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "", method = RequestMethod.POST)
    public ResponseEntity&lt;Delivery&gt; post(@Valid @RequestBody Delivery delivery) throws ApiException {
        try {
            delivery = deliveryRepository.save(delivery);
        } catch (Exception e) {
            throw new ApiException();
        }
        return new ResponseEntity&amp;lt;&amp;gt;(delivery, HttpStatus.OK);
    }&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "/{id}", method = RequestMethod.GET)
    public ResponseEntity&lt;Delivery&gt; get(@PathVariable long id) throws ApiException {
        Optional&lt;Delivery&gt; delivery = deliveryRepository.findById(id);
        if (delivery.isEmpty()) {
            throw new NotFoundException();
        }
        return new ResponseEntity&amp;lt;&amp;gt;(delivery.get(), HttpStatus.OK);
    }
}
```&lt;/p&gt;
&lt;p&gt;We used the annotation&lt;code&gt;@Valid&lt;/code&gt; to ensure that our constraints defined above are met on the sent &lt;em&gt;Delivery&lt;/em&gt; body.&lt;/p&gt;
&lt;p&gt;```shell
$ curl -i "localhost:8080/delivery" -H 'Content-Type: application/json' \
  -X POST -d '{"state": "PENDING"}'                &lt;br /&gt;
HTTP/1.1 400 &lt;/p&gt;
&lt;p&gt;$ curl -i "localhost:8080/delivery/1" -H 'Content-Type: application/json'
HTTP/1.1 404 &lt;/p&gt;
&lt;p _id_:1_state_:_PENDING_location_:_Budapest_="&amp;quot;id&amp;quot;:1,&amp;quot;state&amp;quot;:&amp;quot;PENDING&amp;quot;,&amp;quot;location&amp;quot;:&amp;quot;Budapest&amp;quot;"&gt;$ curl -i "localhost:8080/delivery" -H 'Content-Type: application/json' \
  -X POST -d '{"state": "PENDING", "location":"Budapest"}'
HTTP/1.1 200 &lt;/p&gt;
&lt;p&gt;$ curl -i "localhost:8080/delivery/1" -H 'Content-Type: application/json'                                                                                 130 ↵
HTTP/1.1 200
{"id":1,"state":"PENDING","location":"Budapest"}
```&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: H2 is an in-memory database so the data will be wiped out at each server restart.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation Links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-data/commons/docs/2.4.2/api/org/springframework/data/repository/CrudRepository.html"&gt;CrudRepository API Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#using-boot-spring-beans-and-dependency-injection"&gt;Spring Component Declaration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://javaee.github.io/javaee-spec/javadocs/javax/validation/package-summary.html"&gt;javax.validation API documentation (@Valid)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Paginate the results&lt;/h3&gt;
&lt;p&gt;This section illustrates how well Spring Boot integrates some classic features of a web API. 
To paginate the access to our previous entity &lt;em&gt;Delivery,&lt;/em&gt; we simply have to change the repository’s extended class from &lt;code&gt;CrudRepository&lt;/code&gt; to &lt;code&gt;PagingAndSortingRepository&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
@Repository
public interface DeliveryRepository extends PagingAndSortingRepository&amp;lt;Delivery, Long&amp;gt; {
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This repository implementation provides a new method &lt;code&gt;findAll(Pageable)&lt;/code&gt; returning a &lt;code&gt;Page&lt;/code&gt;. 
The class &lt;code&gt;Pageable&lt;/code&gt; configures the page and page size to return.&lt;/p&gt;
&lt;p&gt;```java
@RestController
@RequestMapping(value = "/delivery",
        consumes = MediaType.APPLICATION_JSON_VALUE,
        produces = MediaType.APPLICATION_JSON_VALUE)
public class DeliveryController {&lt;/p&gt;
&lt;p&gt;private final DeliveryRepository deliveryRepository;&lt;/p&gt;
&lt;p&gt;@Autowired
    public DeliveryController(DeliveryRepository deliveryRepository) {
        this.deliveryRepository = deliveryRepository;
    }&lt;/p&gt;
&lt;p&gt;@RequestMapping(value = "", method = RequestMethod.GET)
    public ResponseEntity&lt;Page\&lt;Delivery&gt;&gt; index(@RequestParam(required = false, defaultValue = "0") int page) {
        Pageable pageable = PageRequest.of(page, 50);
        return new ResponseEntity&amp;lt;&amp;gt;(deliveryRepository.findAll(pageable), HttpStatus.OK);
    }
}
```&lt;/p&gt;
&lt;p&gt;The endpoint will then serve the whole &lt;code&gt;Page&lt;/code&gt; object’s data upon request.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;shell
$ curl "localhost:8080/delivery" -H 'Content-Type: application/json' | jq                                                                                   4 ↵
{
  "content": [
    {
      "id": 1,
      "state": "PENDING",
      "location": "Budapest"
    }
  ],
  "pageable": {
    "sort": {
      "sorted": false,
      "unsorted": true,
      "empty": true
    },
    "offset": 0,
    "pageNumber": 0,
    "pageSize": 50,
    "paged": true,
    "unpaged": false
  },
  "totalPages": 1,
  "totalElements": 1,
  "last": true,
  "first": true,
  "size": 50,
  "number": 0,
  "sort": {
    "sorted": false,
    "unsorted": true,
    "empty": true
  },
  "numberOfElements": 1,
  "empty": false
}&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-data/commons/docs/2.4.2/api/org/springframework/data/repository/PagingAndSortingRepository.html"&gt;PagingAndSortingRepository API Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-data/commons/docs/2.4.2/api/org/springframework/data/domain/PageRequest.html"&gt;PageRequest API Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-data/commons/docs/2.4.2/api/org/springframework/data/domain/Page.html"&gt;Page API Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Test the application&lt;/h3&gt;
&lt;p&gt;Spring Boot provides every tool to easily test controllers with a set of APIs and &lt;a href="https://en.wikipedia.org/wiki/Mock_object"&gt;mocks&lt;/a&gt;. 
Mostly, &lt;code&gt;MockMvc&lt;/code&gt; will enable us to send requests and assert response content without having to worry about technicalities.&lt;/p&gt;
&lt;p&gt;As an example, we are testing the &lt;em&gt;POST&lt;/em&gt; endpoint from the section above. One of these tests is successfully creating a &lt;em&gt;Delivery&lt;/em&gt; entity, and the second one simulates an error coming from the database.&lt;/p&gt;
&lt;p&gt;To avoid relying on a physical instance of a persistence layer, we injected our DeliveryRepository instance using &lt;code&gt;@MockBean&lt;/code&gt;, which creates and injects a mock of our component.&lt;/p&gt;
&lt;p&gt;```java
@SpringBootTest
@AutoConfigureMockMvc
class DeliveryControllerTest {
    @Autowired
    private MockMvc mvc;&lt;/p&gt;
&lt;p&gt;@MockBean
    DeliveryRepository deliveryRepository;&lt;/p&gt;
&lt;p&gt;@Test
    void testPostDeliveryOk() throws Exception {
        ObjectMapper mapper = new ObjectMapper();
        Map&lt;String, String&gt; delivery = getValidDelivery();
        String body = mapper.writeValueAsString(delivery);
        MockHttpServletRequestBuilder accept =
                MockMvcRequestBuilders.post("/delivery")
                        .accept(MediaType.APPLICATION_JSON)
                        .contentType(MediaType.APPLICATION_JSON)
                        .content(body);
        mvc.perform(accept).andExpect(status().isOk());
    }&lt;/p&gt;
&lt;p&gt;@Test
    void testPostPersistIssue() throws Exception {
        ObjectMapper mapper = new ObjectMapper();
        Map&lt;String, String&gt; delivery = getValidDelivery();
        String body = mapper.writeValueAsString(delivery);
        Mockito.when(deliveryRepository.save(Mockito.any())).thenThrow(new RuntimeException());&lt;/p&gt;
&lt;p&gt;MockHttpServletRequestBuilder accept =
                MockMvcRequestBuilders.post("/delivery")
                        .accept(MediaType.APPLICATION_JSON)
                        .contentType(MediaType.APPLICATION_JSON)
                        .content(body);&lt;/p&gt;
&lt;p&gt;mvc.perform(accept).andExpect(status().is4xxClientError());
    }&lt;/p&gt;
&lt;p&gt;private Map&lt;String, String&gt; getValidDelivery() {
        Map&lt;String, String&gt; delivery = new HashMap&amp;lt;&amp;gt;();
        delivery.put("state", "PENDING");
        delivery.put("location", "Rome");
        return delivery;
    }
}
```&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-testing-spring-boot-applications"&gt;@SpringBootTest&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-testing-spring-boot-applications-testing-with-mock-environment"&gt;@AutoConfiguredMockMvc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-testing-spring-boot-applications-mocking-beans"&gt;@MockBean&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/test/web/servlet/MockMvc.html"&gt;MockMvc api Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Package the application&lt;/h3&gt;
&lt;p&gt;Spring boot also eases the application packaging either as a standalone jar or a docker image.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To create a ready to run &lt;em&gt;fat jar&lt;/em&gt;, execute &lt;code&gt;./gradlew bootJar&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To build a &lt;em&gt;docker image&lt;/em&gt;, execute &lt;code&gt;./gradlew bootBuildImage&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that docker does not like uppercase characters in the image name, but we can easily customize the image name and version.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;groovy
// Only use lowercase on docker image name
tasks.named("bootBuildImage") {
    imageName = "${rootProject.name.toLowerCase()}:${version}"
}&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Documentation links:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#getting-started-first-application-executable-jar"&gt;Create an application fat jar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.spring.io/spring-boot/docs/2.4.2/reference/htmlsingle/#boot-features-container-images"&gt;Configure Docker Image&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Spring Boot can be used with a handful of annotations and will manage most of the configuration for you. 
However, most of the configuration can be overridden to provide your own behavior if necessary. 
This makes it a good framework to design proof of concepts while keeping room for optimization if the project grows specific needs.&lt;/p&gt;
&lt;p&gt;If you want to know more about the framework, I can’t stress enough the quality of the &lt;a href="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/"&gt;Reference Documentation&lt;/a&gt;, which gives really good details.&lt;/p&gt;
&lt;p&gt;If you want to play around with some code, you can find all those concepts on an example delivery API &lt;a href="https://github.com/aveuiller/frameworks-bootstrap/tree/master/SpringBoot"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;</content><category term="Software Engineering"></category><category term="Cheat sheet"></category><category term="Java"></category><category term="Spring Boot"></category></entry><entry><title>Introduction to Flaky Tests by Example</title><link href="https://aveuiller.github.io/introduction_to_flaky_tests_by_example.html" rel="alternate"></link><published>2020-08-10T00:00:00+02:00</published><updated>2020-08-10T00:00:00+02:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2020-08-10:/introduction_to_flaky_tests_by_example.html</id><summary type="html">&lt;p&gt;Some real world examples of flaky tests&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Medium: &lt;a href="https://medium.com/@aveuiller/stories-of-flaky-test-encounters-in-the-wild-a152bf7151f5"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img alt="Flakiness effect" src="/images/posts/2020-08-10_Introduction-to-Flaky-Tests-by-Example/red_light.gif" /&gt;&lt;/p&gt;
&lt;p&gt;Tests are an essential part of software development as they give a ground truth about the code sanity. As developers, we reasonably expect our unit tests to give the same results if the source code does not change. It can happen, however, that the result of a unit test changes over multiple executions of a test suite without any change in the code. Such a test is named a flaky test.&lt;/p&gt;
&lt;p&gt;A flaky test is not dangerous &lt;em&gt;per se&lt;/em&gt; but reduces the confidence a developer can give to his test suite, diminishing the benefits of the latter. It is thus recommended to eradicate such issue as soon as possible.&lt;/p&gt;
&lt;p&gt;However, depending on the origin of the flakiness, one may find out only a few days, months or even years later that the tests are flaky.
It may be hard to dive back into those and find the root causes, &lt;a href="https://martinfowler.com/articles/nonDeterminism.html"&gt;so usually, we tend to put those tests aside to make them less annoying or we rerun them until success&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fingers crossed" src="/images/posts/2020-08-10_Introduction-to-Flaky-Tests-by-Example/fingers_crossed.gif" /&gt;&lt;/p&gt;
&lt;p&gt;As a real-world example of flaky tests and the logic behind their resolution. I will talk about two interesting cases I had the opportunity to fix during my career.&lt;/p&gt;
&lt;h3&gt;Storytime!&lt;/h3&gt;
&lt;p&gt;During my career, I stumbled onto a couple of flaky tests issues. There are two instances that, in my opinion, are quite symptomatic of test flakiness, with quite different contexts.&lt;/p&gt;
&lt;p&gt;The examples are voluntary adapted to a simpler context than the original ones to keep a short and focused article.&lt;/p&gt;
&lt;h4&gt;Story 1: 5 days a month isn’t a big deal&lt;/h4&gt;
&lt;p&gt;I entered a project where continuous integration was broken during the 5 first days of each month. 
I was told that this wasn’t a big deal since we don’t need to deploy this project at the beginning of a month.
The priority of fixing those tests was so low that it has remained like this for years before we took the time to tackle this issue.&lt;/p&gt;
&lt;p&gt;```python
def add_data(input_datetime, data):
    # Store the data along with the input date
    # [...]&lt;/p&gt;
&lt;p&gt;def retrieve_data(lower_date):
    # Retrieve all data from lower date to now 
    # [...]&lt;/p&gt;
&lt;p&gt;def compute_stats():
    # Compute some statistics about stored data
    month = arrow.get().floor('month') 
    data = retrieve_data(month)
    return len(data)&lt;/p&gt;
&lt;p&gt;def test_compute_stats():
    # Test method checking the behaviour of compute_stats
    now = arrow.get()&lt;/p&gt;
&lt;p&gt;add_data(now.shift(days=-5))
    add_data(now.shift(days=-1))
    stat = compute_stats()&lt;/p&gt;
&lt;p&gt;assert stat == 2, "We retrieve the two data input"
```
The faulty feature was computing statistics about the current month. As the developer creating the initial tests wanted to take all cases into consideration, he created a test that gave as input multiple dates relative to the current &lt;em&gt;datetime&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Among those inputs, one was &lt;em&gt;5 days before the current date,&lt;/em&gt; and the test was always computing the statistics as if it was part of the same month. As a result, it led to the tests being faulty at the beginning of each month. We can then imagine that the flakiness was detected under one to three weeks after the feature development and from then on, ignored.&lt;/p&gt;
&lt;p&gt;This test is time-dependent because &lt;em&gt;compute_stats&lt;/em&gt; will call for the date of analysis itself. As a result, the computation will always be dependent on the current date. One way of fixing such issues would be to sandbox the execution of the tests in order to control the current date.&lt;/p&gt;
&lt;p&gt;At first, we wanted to rely on &lt;a href="https://medium.com/swlh/about-design-patterns-dependency-injection-ab9c1742d4aa"&gt;dependency injection&lt;/a&gt; and make &lt;em&gt;compute_stats&lt;/em&gt; ask for a month to compute the statistics. This would create an easy way of sandboxing the execution and also potentially open the door to new features. However, in this project, this wasn’t trivial to implement because there was a lot of code dependent on this feature.&lt;/p&gt;
&lt;p&gt;Another way of doing so would be to inject the value directly to the method. Python has a very good library to sandbox the tests when using the built-in &lt;em&gt;datetime&lt;/em&gt; objects: &lt;a href="https://github.com/spulec/freezegun"&gt;freezegun&lt;/a&gt;. Once again, and unfortunately for us, the project was using &lt;a href="https://github.com/crsmithdev/arrow"&gt;arrow&lt;/a&gt; so this was not a possibility.&lt;/p&gt;
&lt;p&gt;Fortunately, and thanks to some previously well-thought environment on the project, we had a central method to provide the current date, which was initially intended to prevent the use of a wrong timezone.&lt;/p&gt;
&lt;p&gt;By mixing this method to the awesome &lt;a href="https://docs.python.org/3/library/unittest.mock.html#the-patchers"&gt;patch decorator&lt;/a&gt; of python mock library (which is part of the standard &lt;em&gt;unittest&lt;/em&gt; library since 3.3), we solved the issue with a simple modification.&lt;/p&gt;
&lt;p&gt;````python
def add_data(input_datetime, data):
    # Store the data along with the input date
    # [...]&lt;/p&gt;
&lt;p&gt;def retrieve_data(lower_date):
    # Retrieve all data from lower date to now 
    # [...]&lt;/p&gt;
&lt;p&gt;def compute_stats():
    # Compute some statistics about stored data
    month = arrow.get().floor('month') 
    data = retrieve_data(month)
    return len(data)&lt;/p&gt;
&lt;p&gt;def test_compute_stats():
    # Test method checking the behaviour of compute_stats
    now = arrow.get()&lt;/p&gt;
&lt;p&gt;add_data(now.shift(days=-5))
    add_data(now.shift(days=-1))
    stat = compute_stats()&lt;/p&gt;
&lt;p&gt;assert stat == 2, "We retrieve the two data input"
````&lt;/p&gt;
&lt;p&gt;By sandboxing the execution to a given point in time, we ensured the reproducibility of the build at any given time.&lt;/p&gt;
&lt;h4&gt;Story 2: We use that configuration!&lt;/h4&gt;
&lt;p&gt;In another project, while creating a new feature we happened to break tests unrelated to our changes. This case could have been tedious to pinpoint, fortunately, due to the project organization, we were certain that the new feature did not affect the code covered by the now failing tests.&lt;/p&gt;
&lt;p&gt;The code below is a synthetic representation of what happened, a global &lt;em&gt;config&lt;/em&gt; object was interacting with both the existing and new features.&lt;/p&gt;
&lt;p&gt;```python&lt;/p&gt;
&lt;h1&gt;Global state configuration&lt;/h1&gt;
&lt;p&gt;config = {}&lt;/p&gt;
&lt;p&gt;def existing_feature():
    if "common_entry" not in config:
        raise ValueError("Not configured")&lt;/p&gt;
&lt;p&gt;# Process [...]
    return True&lt;/p&gt;
&lt;p&gt;def our_new_feature():
    if "common_entry" not in config:
        raise ValueError("Not configured")&lt;/p&gt;
&lt;p&gt;# Process [...]
    return True
```&lt;/p&gt;
&lt;p&gt;From the isolation of the two features, we knew that the new tests had to be the ones creating a faulty global state. There were globally two possibilities for the faulty state.
Either the new test was injecting something new to the global state, or removing something essential to the existing test.&lt;/p&gt;
&lt;p&gt;The test cases below were always run in the specific order &lt;em&gt;ConfiguredFeatureTest, ExistingFeatureTestCase&lt;/em&gt; before integrating the new feature, then in the order &lt;em&gt;ConfiguredFeatureTest, NewFeatureTestCase, ExistingFeatureTestCase.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;````python
class ConfiguredFeatureTest(unittest.TestCase):
    def setUp(self):
        config["entry"] = "anything"&lt;/p&gt;
&lt;p&gt;def test_configured(self):
        self.assertIsNotNone(config.get("entry"))&lt;/p&gt;
&lt;p&gt;class ExistingFeatureTestCase(unittest.TestCase):
    def test_feature_one(self):
        self.assertTrue(existing_feature())&lt;/p&gt;
&lt;p&gt;class NewFeatureTestCase(unittest.TestCase):
    def setUp(self):
        config["entry"] = "anything"&lt;/p&gt;
&lt;p&gt;def tearDown(self):
        config.clear()&lt;/p&gt;
&lt;p&gt;def test_new_feature(self):
        self.assertTrue(our_new_feature())
````&lt;/p&gt;
&lt;p&gt;In order to understand the behaviour of the existing test, we ran it alone, both with and without the new changes. It appeared that the test was failing in both cases. This gave us the information that this test was using an existing global state, and that we might be cleaning this state. So we took a deeper interest in the &lt;em&gt;tearDown&lt;/em&gt; method.&lt;/p&gt;
&lt;p&gt;It happened that the global configuration was injected and cleared in our new test suite. This configuration was used but rarely cleared in other tests. As a result, the existing test was relying on the execution of the previous ones to succeed. Clearing the configuration removed the context required by the existing test, thus made it fail.&lt;/p&gt;
&lt;p&gt;By “chance” the tests were always run in the right order for years. This situation could have been detected way earlier by using a random execution order for tests. &lt;a href="https://github.com/pytest-dev/pytest-randomly"&gt;It happens that python has simple modules to do so&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To fix the tests and avoid this situation to happen in the future, we decided to force the configuration clearing in the project’s test suite superclass. This meant to fix a bunch of other tests failing after this but also enforced a clean state for all new tests.&lt;/p&gt;
&lt;h4&gt;Bonus story: A good flakiness&lt;/h4&gt;
&lt;p&gt;On top of the previous stories, where flakiness is obviously a bad thing, I also stumbled into a case where I found flakiness somehow beneficial to the codebase.&lt;/p&gt;
&lt;p&gt;In this particular case, the test intended to assert some data consistency for any instances of the same class. To do so, the test was generating numerous instances of the class with randomized inputs.&lt;/p&gt;
&lt;p&gt;This test happened to fail during some executions as the inputs were creating a behaviour not accounted for by the feature. That enabled to extract a specific test case for the input and fix the behaviour in this case.&lt;/p&gt;
&lt;p&gt;While I agree that edge cases should be analyzed during the development process, sometimes the input scope is too wide to consider all of them, let alone test all possibilities.&lt;/p&gt;
&lt;p&gt;In those cases, randomizing the input of a method that should keep a consistent output is a good way to assert the codebase sanity in the long run.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This article showed some real-life examples of flaky tests. They were not the worst to track down, but they pinpoint the fact that flakiness can resurface at any time, even years after their introduction!&lt;/p&gt;
&lt;p&gt;Once they appear, the flaky tests need to be fixed as soon as possible out of fear that the failing test suite will be considered as a normal state. The developers may then not rely on the tests suites anymore.&lt;/p&gt;
&lt;p&gt;Flakiness is mostly due to non-deterministic behaviour in the code, in this article we had an example of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Specific execution time&lt;/em&gt;. If the code is dependent on time, there may be failing tests at specific dates.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Randomness.&lt;/em&gt; Using random values in the main code or in the tests needs extra care or the behaviour may vary depending on those random values.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Modified global state.&lt;/em&gt; Using a global state in a project can create inconsistencies in tests if the state is not managed correctly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some behaviour can help to limit the amount of flakiness that hides in the tests, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Control your test execution environment&lt;/em&gt; to keep reproducible execution.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Avoid global states&lt;/em&gt; to minimize the side effects of environment settings.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Randomize the test execution order&lt;/em&gt; to determine dependencies between tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Continue on the subject:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://hackernoon.com/flaky-tests-a-war-that-never-ends-9aa32fdef359"&gt;https://hackernoon.com/flaky-tests-a-war-that-never-ends-9aa32fdef359&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martinfowler.com/articles/nonDeterminism.html"&gt;https://martinfowler.com/articles/nonDeterminism.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytest.org/en/stable/flaky.html"&gt;https://docs.pytest.org/en/stable/flaky.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are curious about the context that led to the apparition of those flaky tests, my former manager &lt;a href="https://kevin.deldycke.com/"&gt;Kevin Deldycke&lt;/a&gt; provides a more detailed view in a very interesting post: &lt;a href="https://kevin.deldycke.com/2020/10/billing-pipeline-critical-time-sensitive-system/"&gt;&lt;em&gt;Billing Pipeline: A Critical Time Sensitive System&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;</content><category term="Software Engineering"></category><category term="Testing"></category><category term="Best Practice"></category></entry><entry><title>About design patterns: Dependency Injection</title><link href="https://aveuiller.github.io/about_design_patterns-dependency_injection.html" rel="alternate"></link><published>2020-01-05T00:00:00+01:00</published><updated>2020-01-05T00:00:00+01:00</updated><author><name>Antoine Veuiller</name></author><id>tag:aveuiller.github.io,2020-01-05:/about_design_patterns-dependency_injection.html</id><summary type="html">&lt;p&gt;A walk through dependency injection&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Availability Disclaimer&lt;/h3&gt;
&lt;p&gt;This article can be found on other sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Medium: &lt;a href="https://medium.com/@aveuiller/about-design-patterns-dependency-injection-ab9c1742d4aa"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img alt="credit photo: Bamboo Complexity, taufuuu" src="/images/posts/2020-01-05_About-design-patterns--Dependency-Injection/bamboo_complexity.jpeg" /&gt;&lt;/p&gt;
&lt;h3&gt;What is dependency injection?&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Dependency injection&lt;/em&gt; (DI) is a very simple concept that aims to decouple components of your software and ease their integration and testing. It does so by asking for their sub-components instead of creating them.&lt;/p&gt;
&lt;p&gt;During this article, we will also mention &lt;em&gt;inversion of control&lt;/em&gt; (IoC), which is commonly used along with dependency injection. This pattern aims to avoid asking for implementations but rather interfaces while injecting dependencies.&lt;/p&gt;
&lt;p&gt;This article will use a simple example in Java to present dependency injection but aims towards a technology-agnostic explanation of the concept and its advantages. Moreover, even if it is an object-oriented design pattern, you can still adapt the behaviour in many programming languages.&lt;/p&gt;
&lt;h3&gt;Let’s clarify all this using an example!&lt;/h3&gt;
&lt;p&gt;We will present a weather service that shows an intelligible representation of the weather. In the current implementation, we rely solely on a thermometer.&lt;/p&gt;
&lt;h4&gt;Let’s start without dependency injection.&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Weather service without IoC" src="https://raw.githubusercontent.com/aveuiller/design-tutorials/master/dependencyinjection/specs/classes_without_ioc.svg" /&gt;&lt;/p&gt;
&lt;p&gt;As you can see on the diagram, the &lt;em&gt;WeatherService&lt;/em&gt; is relying on a &lt;em&gt;Thermometer&lt;/em&gt;, which can be configured with a &lt;em&gt;TemperatureUnit&lt;/em&gt;. 
Not using dependency injection will result in a code creating a new instance of &lt;em&gt;Thermometer&lt;/em&gt; in the service, and a &lt;em&gt;Thermometer&lt;/em&gt; configuring the &lt;em&gt;TemperatureUnit&lt;/em&gt; to use:&lt;/p&gt;
&lt;p&gt;````java
public class Thermometer {
  private final TemperatureUnit unit;&lt;/p&gt;
&lt;p&gt;public Thermometer() {
    this.unit = TemperatureUnit.CELSIUS;
  }
}&lt;/p&gt;
&lt;p&gt;public class WeatherService implements WeatherContract {&lt;/p&gt;
&lt;p&gt;private final Thermometer thermometer;&lt;/p&gt;
&lt;p&gt;// This constructor is not using dependency injection
  public WeatherService() {
    this.thermometer = new Thermometer();
  }
}
````
Now let’s imagine that we want to use a &lt;em&gt;Thermometer&lt;/em&gt; configured to use Fahrenheit degrees instead of Celsius. For this, we add a parameter to switch between both units.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
public Thermometer(boolean useCelsius) {
  if (useCelsius) {
    this.unit = TemperatureUnit.CELSIUS;
  } else {
    this.unit = TemperatureUnit.FAHRENHEIT;
  }
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;One can also argue that the user of our program won’t always have access to an actual thermometer on their device, thus you may want to be able to fall back to another implementation in this case. 
For instance, an API sending the current temperature in your area. Integrating multiple implementations inside the service could be done as shown below.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
public WeatherService(boolean useRealDevice, 
                      boolean useCelsius,
                      String apiKey) {
  if (useRealDevice) {
    this.thermometer = new Thermometer(useCelsius);
  } else {
    this.thermometer = new ThermometerWebService(useCelsius, apiKey);
  }
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As a result, initializing the service can be done as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
public static void main(String[] args) {
  // Not using dependency injection
  WeatherContract weather = new WeatherService(true, true, null);
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Even if it is easy to use, our current version of the &lt;em&gt;WeatherService&lt;/em&gt; is not evolutive. If we take a closer look at its constructor, we can see multiple design flaws that will haunt us in the long run:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The constructor is choosing its &lt;em&gt;Thermometer&lt;/em&gt;. Adding a new type of Thermometer would require some parameter tricks to guess the implementation to use.&lt;/li&gt;
&lt;li&gt;The constructor is managing the &lt;em&gt;Thermometer&lt;/em&gt; constructor parameters. Adding the &lt;em&gt;ThermometerWebService&lt;/em&gt; forced us to add a new &lt;em&gt;apiKey&lt;/em&gt; parameter to it, even if unrelated to the &lt;em&gt;WeatherService&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, any change to any &lt;em&gt;Thermometer&lt;/em&gt; implementation may require changes on the &lt;em&gt;WeatherService&lt;/em&gt; constructors. This behaviour is unwanted and breaks the &lt;em&gt;Separation of Concerns&lt;/em&gt; principle.&lt;/p&gt;
&lt;h4&gt;Will dependency injection improve my project?&lt;/h4&gt;
&lt;p&gt;Dependency injection, associated with inversion of control, is a good way to cover this use case. It allows you to choose which kind of thermometer you want in your program depending on the situation. The following diagram gives a quick overview of our new architecture:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Weather service using IoC" src="https://raw.githubusercontent.com/aveuiller/design-tutorials/master/dependencyinjection/specs/classes_using_ioc.svg" /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;inversion of control&lt;/strong&gt; is represented in this diagram by the fact that our &lt;em&gt;WeatherService&lt;/em&gt; implementation is linked to &lt;em&gt;ThermometerContract&lt;/em&gt; rather than any of its implementations. That’s nothing more than this.&lt;/p&gt;
&lt;p&gt;As for &lt;strong&gt;dependency injection&lt;/strong&gt;, &lt;em&gt;WeatherService&lt;/em&gt; will now take a &lt;em&gt;ThermometerContract&lt;/em&gt; in its constructor, requiring the block using the service to build an instance filling this contract:&lt;/p&gt;
&lt;p&gt;````java
public class WeatherService implements WeatherContract {
  // We now use the Interface &lt;br /&gt;
  private final ThermometerContract thermometer;&lt;/p&gt;
&lt;p&gt;// New constructor using dependency injection  &lt;br /&gt;
  public WeatherService(ThermometerContract thermometer) {
    this.thermometer = thermometer;
  }
}
````&lt;/p&gt;
&lt;p&gt;As a result, the initialization of a &lt;em&gt;WeatherService&lt;/em&gt; for both constructors will look like the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;java
public static void main(String[] args) {
  // Using dependency injection
  TemperatureUnit celsius = TemperatureUnit.CELSIUS;
  ThermometerContract thermometer = new Thermometer(celsius);
  WeatherContract weather = new WeatherService(thermometer);
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now, our &lt;em&gt;ThermometerContract&lt;/em&gt; can be fully configured by an external part of the software. More important so, the &lt;em&gt;WeatherService&lt;/em&gt; doesn’t need to know any of the available implementations of &lt;em&gt;ThermometerContract&lt;/em&gt;, thus decoupling your software packages.&lt;/p&gt;
&lt;p&gt;This could seem like nothing important, but this simple switch of responsibility is critical leverage for multiple aspects of software design. It enables you to control the instance creation from your software entry point by chaining dependencies. You won’t have to take care of the instantiation until it is necessary. This behaviour could be compared to raised exceptions, that are ignored until taken care of in a significant context.&lt;/p&gt;
&lt;h3&gt;That’s all there is to dependency injection?&lt;/h3&gt;
&lt;p&gt;It is important to know that even if you can find libraries that help you manage your dependency injection, it is not always necessary to use them.&lt;/p&gt;
&lt;p&gt;Those libraries tend to cover a lot of cases thus be offputting to developers not comfortable with the pattern in the first place. In reality, they simply ease the instantiation of complex dependency trees and are not required at all.&lt;/p&gt;
&lt;p&gt;The following section is an example of injecting our service using &lt;a href="https://github.com/google/guice/wiki"&gt;Guice&lt;/a&gt;, a dependency injection framework for Java made by Google. The concept is to reference bindings of every component you can inject in your program, so that the library can generate a class of any type, automatically.&lt;/p&gt;
&lt;p&gt;Let’s consider that we have two implementations with the following constructors:&lt;/p&gt;
&lt;p&gt;```java
public class WeatherService implements WeatherContract {
  private final ThermometerContract thermometer;&lt;/p&gt;
&lt;p&gt;@Inject
  public WeatherService(ThermometerContract thermometer) {
    this.thermometer = thermometer;
  }
}&lt;/p&gt;
&lt;p&gt;public class Thermometer implements ThermometerContract {
  private final TemperatureUnit unit;&lt;/p&gt;
&lt;p&gt;@Inject
  public Thermometer(@Named(WeatherModule.TEMPERATURE_UNIT) 
                     TemperatureUnit unit) {
    this.unit = unit;
  }
}
```&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;injection module&lt;/em&gt; should be configured to bind all needed interfaces to a given implementation.
It should also be able to inject any object without a specific interface, such as the enumerate &lt;em&gt;TemperatureUnit&lt;/em&gt;. 
The injection will then be bound to a specific name, “&lt;em&gt;temp_unit”&lt;/em&gt; in this case.&lt;/p&gt;
&lt;p&gt;````java
public class WeatherModule extends AbstractModule {
  public static final String TEMPERATURE_UNIT = "temp_unit";&lt;/p&gt;
&lt;p&gt;@Override
  protected void configure() {
    // Named input configuration bindings
    bind(TemperatureUnit.class)
      .annotatedWith(Names.named(TEMPERATURE_UNIT))
      .toInstance(TemperatureUnit.CELSIUS);&lt;/p&gt;
&lt;p&gt;// Interface - Implementation bindings
    bind(ThermometerContract.class).to(Thermometer.class);
    bind(WeatherContract.class).to(WeatherService.class);
  }
}
````&lt;/p&gt;
&lt;p&gt;Ultimately, the module can be used as follow, here instantiating a &lt;em&gt;WeatherContract&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;```java
public static void main(String[] args) {
  // Creating the injection module configured above.
  Injector injector = Guice.createInjector(new WeatherModule());&lt;/p&gt;
&lt;p&gt;// We ask for the injection of a WeatherContract, 
  // which will create an instance of ThermometerContract
  // with the named TemperatureUnit under the hood.
  WeatherContract weather = injector.getInstance(WeatherContract.class);
}
```&lt;/p&gt;
&lt;p&gt;Such modules usually provide a good power of customization to the injected elements, thus we can consider configuring the injection depending on the available implementations.&lt;/p&gt;
&lt;p&gt;As a result, using a library is not required when integrating dependency injection. However, this could save a lot of time and cumbersome code in big projects.&lt;/p&gt;
&lt;h3&gt;Show me some tests!&lt;/h3&gt;
&lt;p&gt;As a side effect of decoupling your code, the dependency injection pattern is a real asset to improve unit testability of each component. This section contains an example of unit tests for our &lt;em&gt;WeatherService&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As said above, making &lt;em&gt;WeatherService&lt;/em&gt; asking for a &lt;em&gt;ThermometerContract&lt;/em&gt; enables us to use any implementation we want. Hence, we can send a &lt;em&gt;mock&lt;/em&gt; in the constructor, then control its behaviour from the outside.&lt;/p&gt;
&lt;p&gt;```java
public void testTemperatureStatus() {
  ThermometerContract thermometer = Mockito.mock(ThermometerContract.class);
  Mockito.doReturn(TemperatureUnit.CELSIUS).when(thermometer).getUnit();
  WeatherContract weather = new WeatherService(thermometer);&lt;/p&gt;
&lt;p&gt;Mockito.doReturn(-50f).when(thermometer).getTemperature();
  assertEquals(
    TemperatureStatus.COLD,
    weather.getTemperatureStatus()
  );&lt;/p&gt;
&lt;p&gt;Mockito.doReturn(10f).when(thermometer).getTemperature();
  assertEquals(
    TemperatureStatus.MODERATE,
    weather.getTemperatureStatus()
  );
}
```&lt;/p&gt;
&lt;p&gt;As you can see, we can then control our thermometer without a struggle from outside our tested class.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Dependency injection&lt;/em&gt; is a way of thinking your code architecture and can be simple to implement by yourself. In bigger projects, integrating a dependency injection framework can save you a lot of time in the long run.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Dependency injection&lt;/em&gt; provides multiple non-negligible advantages such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Code decoupling&lt;/em&gt;: use the contracts and ignore implementation specificities.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Enhanced testability&lt;/em&gt;: Unit tests almost become a pleasure to write.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Configurability&lt;/em&gt;: you can more easily swap injected instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/aveuiller/design-tutorials"&gt;You can find the full code example in my design tutorials repository on GitHub&lt;/a&gt;&lt;/p&gt;</content><category term="Software Engineering"></category><category term="Design Patterns"></category><category term="Best Practice"></category><category term="Software Architecture"></category></entry></feed>